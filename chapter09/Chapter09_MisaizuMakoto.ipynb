{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa0d1913970>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "from random import shuffle\n",
    "from itertools import chain\n",
    "from collections import Counter, defaultdict\n",
    "from string import ascii_letters as alu # ascii_lowercase + ascii_uppercase\n",
    "from string import ascii_lowercase as al # abcdefghijklmnopqrstuvwxyz\n",
    "from string import ascii_uppercase as au # ABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "from string import digits # '0123456789'\n",
    "from string import hexdigits # '0123456789abcdefABCDEF'\n",
    "from string import octdigits # '01234567'\n",
    "from string import punctuation # !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.keyedvectors import KeyedVectors as Vectors\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt') as f:\n",
    "  train = [l.rstrip() for l in f.readlines()]\n",
    "\n",
    "with open('valid.txt') as f:\n",
    "  valid = [l.rstrip() for l in f.readlines()]\n",
    "\n",
    "with open('test.txt') as f:\n",
    "  test = [l.rstrip() for l in f.readlines()]\n",
    "\n",
    "for data in [train, valid, test]:\n",
    "  for i in range(len(data)):\n",
    "    data[i] = data[i].split('\\t')\n",
    "\n",
    "columns = ['id', 'title', 'url', 'publisher', 'category', 'story', 'hostname', 'timestamp']\n",
    "num_cols = ['id', 'timestamp']\n",
    "\n",
    "train_df = pl.DataFrame(train, orient='row', schema=columns)\n",
    "train_df = train_df.with_columns([pl.col(col).cast(int) for col in num_cols])\n",
    "\n",
    "valid_df = pl.DataFrame(valid, orient='row', schema=columns)\n",
    "valid_df = valid_df.with_columns([pl.col(col).cast(int) for col in num_cols])\n",
    "\n",
    "test_df = pl.DataFrame(test, orient='row', schema=columns)\n",
    "test_df = test_df.with_columns([pl.col(col).cast(int) for col in num_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'to': 1,\n",
       "             'in': 2,\n",
       "             'on': 3,\n",
       "             'as': 4,\n",
       "             'UPDATE': 5,\n",
       "             'for': 6,\n",
       "             'of': 7,\n",
       "             'The': 8,\n",
       "             'US': 9,\n",
       "             'To': 10,\n",
       "             'the': 11,\n",
       "             'and': 12,\n",
       "             'In': 13,\n",
       "             'Of': 14,\n",
       "             'at': 15,\n",
       "             'a': 16,\n",
       "             'With': 17,\n",
       "             'Is': 18,\n",
       "             'A': 19,\n",
       "             'For': 20,\n",
       "             'with': 21,\n",
       "             'And': 22,\n",
       "             'after': 23,\n",
       "             'New': 24,\n",
       "             'Kardashian': 25,\n",
       "             'On': 26,\n",
       "             'by': 27,\n",
       "             'Kim': 28,\n",
       "             'After': 29,\n",
       "             'up': 30,\n",
       "             'says': 31,\n",
       "             '1': 32,\n",
       "             'is': 33,\n",
       "             'At': 34,\n",
       "             'China': 35,\n",
       "             'From': 36,\n",
       "             'new': 37,\n",
       "             'from': 38,\n",
       "             '2': 39,\n",
       "             'Says': 40,\n",
       "             'her': 41,\n",
       "             'ECB': 42,\n",
       "             'Miley': 43,\n",
       "             'shares': 44,\n",
       "             'Cyrus': 45,\n",
       "             'Fed': 46,\n",
       "             'data': 47,\n",
       "             'May': 48,\n",
       "             'CEO': 49,\n",
       "             'St': 50,\n",
       "             'over': 51,\n",
       "             'About': 52,\n",
       "             'First': 53,\n",
       "             'Chris': 54,\n",
       "             'Will': 55,\n",
       "             'West': 56,\n",
       "             'bln': 57,\n",
       "             'Over': 58,\n",
       "             'More': 59,\n",
       "             'Kanye': 60,\n",
       "             'You': 61,\n",
       "             'As': 62,\n",
       "             'Be': 63,\n",
       "             'Justin': 64,\n",
       "             'Ukraine': 65,\n",
       "             'she': 66,\n",
       "             'Up': 67,\n",
       "             'be': 68,\n",
       "             'Its': 69,\n",
       "             'off': 70,\n",
       "             '2014': 71,\n",
       "             'Billion': 72,\n",
       "             'Google': 73,\n",
       "             'Are': 74,\n",
       "             'Bieber': 75,\n",
       "             'Star': 76,\n",
       "             'euro': 77,\n",
       "             'Stocks': 78,\n",
       "             'It': 79,\n",
       "             'but': 80,\n",
       "             'How': 81,\n",
       "             'GLOBAL': 82,\n",
       "             'profit': 83,\n",
       "             'Bank': 84,\n",
       "             'sales': 85,\n",
       "             'Not': 86,\n",
       "             'are': 87,\n",
       "             'out': 88,\n",
       "             'deal': 89,\n",
       "             'pct': 90,\n",
       "             'rise': 91,\n",
       "             'that': 92,\n",
       "             'Her': 93,\n",
       "             'Day': 94,\n",
       "             'more': 95,\n",
       "             'This': 96,\n",
       "             'STOCKSWall': 97,\n",
       "             'That': 98,\n",
       "             'growth': 99,\n",
       "             'Game': 100,\n",
       "             'By': 101,\n",
       "             'Out': 102,\n",
       "             'not': 103,\n",
       "             'he': 104,\n",
       "             '3': 105,\n",
       "             'UK': 106,\n",
       "             'low': 107,\n",
       "             'Apple': 108,\n",
       "             'Deal': 109,\n",
       "             'Twitter': 110,\n",
       "             'first': 111,\n",
       "             'it': 112,\n",
       "             'near': 113,\n",
       "             'I': 114,\n",
       "             'IPO': 115,\n",
       "             'Lindsay': 116,\n",
       "             'Sales': 117,\n",
       "             'Time': 118,\n",
       "             'What': 119,\n",
       "             'Beyonce': 120,\n",
       "             'Euro': 121,\n",
       "             'Your': 122,\n",
       "             'down': 123,\n",
       "             'FOREXDollar': 124,\n",
       "             'could': 125,\n",
       "             'his': 126,\n",
       "             'Million': 127,\n",
       "             'Most': 128,\n",
       "             'set': 129,\n",
       "             'will': 130,\n",
       "             'American': 131,\n",
       "             'high': 132,\n",
       "             'higher': 133,\n",
       "             'Home': 134,\n",
       "             'Movie': 135,\n",
       "             'inflation': 136,\n",
       "             'GM': 137,\n",
       "             'Thrones': 138,\n",
       "             'buy': 139,\n",
       "             'York': 140,\n",
       "             'Selena': 141,\n",
       "             'Video': 142,\n",
       "             'fall': 143,\n",
       "             'stocks': 144,\n",
       "             'reveals': 145,\n",
       "             'was': 146,\n",
       "             'America': 147,\n",
       "             'One': 148,\n",
       "             'Paul': 149,\n",
       "             'Why': 150,\n",
       "             'has': 151,\n",
       "             'rises': 152,\n",
       "             'All': 153,\n",
       "             'Yellen': 154,\n",
       "             'His': 155,\n",
       "             'No': 156,\n",
       "             'George': 157,\n",
       "             'Show': 158,\n",
       "             'about': 159,\n",
       "             'ahead': 160,\n",
       "             'Has': 161,\n",
       "             'shows': 162,\n",
       "             'Facebook': 163,\n",
       "             'Michael': 164,\n",
       "             'Report': 165,\n",
       "             'WRAPUP': 166,\n",
       "             'oil': 167,\n",
       "             '4': 168,\n",
       "             'Lohan': 169,\n",
       "             'falls': 170,\n",
       "             'Awards': 171,\n",
       "             'Iraq': 172,\n",
       "             'Jennifer': 173,\n",
       "             'Rise': 174,\n",
       "             'Before': 175,\n",
       "             'European': 176,\n",
       "             'Gomez': 177,\n",
       "             'Have': 178,\n",
       "             'Jay': 179,\n",
       "             'Outlook': 180,\n",
       "             'Profit': 181,\n",
       "             'TV': 182,\n",
       "             'Was': 183,\n",
       "             'may': 184,\n",
       "             'record': 185,\n",
       "             'High': 186,\n",
       "             'Just': 187,\n",
       "             'Than': 188,\n",
       "             'Ebola': 189,\n",
       "             'June': 190,\n",
       "             'rate': 191,\n",
       "             'James': 192,\n",
       "             'Said': 193,\n",
       "             'Draghi': 194,\n",
       "             'Get': 195,\n",
       "             'gains': 196,\n",
       "             'lower': 197,\n",
       "             'SP': 198,\n",
       "             'Amazon': 199,\n",
       "             'But': 200,\n",
       "             'Wars': 201,\n",
       "             'have': 202,\n",
       "             'into': 203,\n",
       "             'wedding': 204,\n",
       "             '500': 205,\n",
       "             'Amid': 206,\n",
       "             'Could': 207,\n",
       "             'Dollar': 208,\n",
       "             'Drop': 209,\n",
       "             'Falls': 210,\n",
       "             'Into': 211,\n",
       "             'mln': 212,\n",
       "             '10': 213,\n",
       "             'BNP': 214,\n",
       "             'Big': 215,\n",
       "             'Low': 216,\n",
       "             'prices': 217,\n",
       "             'Years': 218,\n",
       "             'Z': 219,\n",
       "             'hit': 220,\n",
       "             'jobs': 221,\n",
       "             'than': 222,\n",
       "             'Europe': 223,\n",
       "             'Street': 224,\n",
       "             'We': 225,\n",
       "             'against': 226,\n",
       "             '1US': 227,\n",
       "             'Met': 228,\n",
       "             'Robert': 229,\n",
       "             'Wedding': 230,\n",
       "             'Film': 231,\n",
       "             'Pay': 232,\n",
       "             'Top': 233,\n",
       "             'hits': 234,\n",
       "             'zone': 235,\n",
       "             'Death': 236,\n",
       "             'Estimates': 237,\n",
       "             'PRECIOUSGold': 238,\n",
       "             'sees': 239,\n",
       "             '5': 240,\n",
       "             'Season': 241,\n",
       "             'before': 242,\n",
       "             'dollar': 243,\n",
       "             'its': 244,\n",
       "             'revenue': 245,\n",
       "             'Data': 246,\n",
       "             'Music': 247,\n",
       "             'Williams': 248,\n",
       "             'World': 249,\n",
       "             'German': 250,\n",
       "             'Hong': 251,\n",
       "             'Kong': 252,\n",
       "             'Martin': 253,\n",
       "             'Shows': 254,\n",
       "             'Since': 255,\n",
       "             'talks': 256,\n",
       "             'who': 257,\n",
       "             'Can': 258,\n",
       "             'Fitch': 259,\n",
       "             'Love': 260,\n",
       "             'Record': 261,\n",
       "             'flat': 262,\n",
       "             'show': 263,\n",
       "             'Affirms': 264,\n",
       "             'April': 265,\n",
       "             'Gwyneth': 266,\n",
       "             'Jessica': 267,\n",
       "             'North': 268,\n",
       "             'back': 269,\n",
       "             'business': 270,\n",
       "             'can': 271,\n",
       "             'years': 272,\n",
       "             'Change': 273,\n",
       "             'Climate': 274,\n",
       "             'Internet': 275,\n",
       "             'Rate': 276,\n",
       "             'Study': 277,\n",
       "             'day': 278,\n",
       "             'no': 279,\n",
       "             '2US': 280,\n",
       "             '7': 281,\n",
       "             'Angelina': 282,\n",
       "             'Growth': 283,\n",
       "             'Health': 284,\n",
       "             'Netflix': 285,\n",
       "             'Russia': 286,\n",
       "             'STOCKS': 287,\n",
       "             'Two': 288,\n",
       "             'Zac': 289,\n",
       "             'bid': 290,\n",
       "             'bond': 291,\n",
       "             'court': 292,\n",
       "             'cuts': 293,\n",
       "             'drug': 294,\n",
       "             'Alibaba': 295,\n",
       "             'Bonds': 296,\n",
       "             'Men': 297,\n",
       "             'Obama': 298,\n",
       "             'People': 299,\n",
       "             'Review': 300,\n",
       "             'Robin': 301,\n",
       "             'Samsung': 302,\n",
       "             'Set': 303,\n",
       "             'Who': 304,\n",
       "             'market': 305,\n",
       "             'top': 306,\n",
       "             'Episode': 307,\n",
       "             'FOREXEuro': 308,\n",
       "             'Second': 309,\n",
       "             'Shares': 310,\n",
       "             'gets': 311,\n",
       "             'pay': 312,\n",
       "             'Against': 313,\n",
       "             'Buy': 314,\n",
       "             'David': 315,\n",
       "             'Dies': 316,\n",
       "             'Drops': 317,\n",
       "             'Emma': 318,\n",
       "             'Japan': 319,\n",
       "             'March': 320,\n",
       "             'Mick': 321,\n",
       "             'Off': 322,\n",
       "             'Rises': 323,\n",
       "             'year': 324,\n",
       "             'Ford': 325,\n",
       "             'French': 326,\n",
       "             'Gain': 327,\n",
       "             'Gains': 328,\n",
       "             'Jackson': 329,\n",
       "             'Obamacare': 330,\n",
       "             'Seen': 331,\n",
       "             'Still': 332,\n",
       "             'Talks': 333,\n",
       "             'Week': 334,\n",
       "             'banks': 335,\n",
       "             'million': 336,\n",
       "             'report': 337,\n",
       "             'takes': 338,\n",
       "             'Brown': 339,\n",
       "             'Johnny': 340,\n",
       "             'Jolie': 341,\n",
       "             'Stars': 342,\n",
       "             'economy': 343,\n",
       "             'Court': 344,\n",
       "             'Credit': 345,\n",
       "             'Jagger': 346,\n",
       "             'Now': 347,\n",
       "             'Oil': 348,\n",
       "             'Paltrow': 349,\n",
       "             'Trailer': 350,\n",
       "             'bank': 351,\n",
       "             'percent': 352,\n",
       "             'two': 353,\n",
       "             'AstraZeneca': 354,\n",
       "             'Best': 355,\n",
       "             'Efron': 356,\n",
       "             'House': 357,\n",
       "             'Pfizer': 358,\n",
       "             'Taylor': 359,\n",
       "             'strong': 360,\n",
       "             'time': 361,\n",
       "             'yields': 362,\n",
       "             'Back': 363,\n",
       "             'Dont': 364,\n",
       "             'Fans': 365,\n",
       "             'Jenner': 366,\n",
       "             'News': 367,\n",
       "             'Ryan': 368,\n",
       "             'an': 369,\n",
       "             'earnings': 370,\n",
       "             'ends': 371,\n",
       "             'star': 372,\n",
       "             'their': 373,\n",
       "             'Brad': 374,\n",
       "             'Down': 375,\n",
       "             'Market': 376,\n",
       "             'STOCKSFutures': 377,\n",
       "             'White': 378,\n",
       "             'claims': 379,\n",
       "             'home': 380,\n",
       "             'offer': 381,\n",
       "             'since': 382,\n",
       "             'take': 383,\n",
       "             'week': 384,\n",
       "             'An': 385,\n",
       "             'Economy': 386,\n",
       "             'FDA': 387,\n",
       "             'Scott': 388,\n",
       "             'So': 389,\n",
       "             'Wall': 390,\n",
       "             'billion': 391,\n",
       "             'comments': 392,\n",
       "             'debt': 393,\n",
       "             'gas': 394,\n",
       "             'open': 395,\n",
       "             'seen': 396,\n",
       "             'source': 397,\n",
       "             '100': 398,\n",
       "             '2015': 399,\n",
       "             'Chinas': 400,\n",
       "             'Gold': 401,\n",
       "             'Know': 402,\n",
       "             'My': 403,\n",
       "             'case': 404,\n",
       "             'AP': 405,\n",
       "             'Festival': 406,\n",
       "             'John': 407,\n",
       "             'MERS': 408,\n",
       "             'Real': 409,\n",
       "             'Some': 410,\n",
       "             'drops': 411,\n",
       "             'rates': 412,\n",
       "             '—': 413,\n",
       "             'Alstom': 414,\n",
       "             'End': 415,\n",
       "             'Fox': 416,\n",
       "             'Harris': 417,\n",
       "             'Khloe': 418,\n",
       "             'Make': 419,\n",
       "             'Plan': 420,\n",
       "             'Risk': 421,\n",
       "             'Thicke': 422,\n",
       "             'Tour': 423,\n",
       "             'firm': 424,\n",
       "             'just': 425,\n",
       "             'slips': 426,\n",
       "             'Air': 427,\n",
       "             'Box': 428,\n",
       "             'Cancer': 429,\n",
       "             'Dead': 430,\n",
       "             'Fall': 431,\n",
       "             'Feds': 432,\n",
       "             'Gets': 433,\n",
       "             'Microsoft': 434,\n",
       "             'Morgan': 435,\n",
       "             'Sees': 436,\n",
       "             'cut': 437,\n",
       "             'policy': 438,\n",
       "             'still': 439,\n",
       "             'they': 440,\n",
       "             'Chinese': 441,\n",
       "             'Inflation': 442,\n",
       "             'Kate': 443,\n",
       "             'Office': 444,\n",
       "             'Pregnant': 445,\n",
       "             'Seth': 446,\n",
       "             'South': 447,\n",
       "             'Stone': 448,\n",
       "             'Warner': 449,\n",
       "             'Young': 450,\n",
       "             'all': 451,\n",
       "             'make': 452,\n",
       "             'you': 453,\n",
       "             '6': 454,\n",
       "             'Cannes': 455,\n",
       "             'EU': 456,\n",
       "             'Good': 457,\n",
       "             'He': 458,\n",
       "             'Life': 459,\n",
       "             'Pitt': 460,\n",
       "             'end': 461,\n",
       "             'makes': 462,\n",
       "             'steady': 463,\n",
       "             'Baby': 464,\n",
       "             'Banks': 465,\n",
       "             'Calls': 466,\n",
       "             'Forecast': 467,\n",
       "             'Future': 468,\n",
       "             'Harrison': 469,\n",
       "             'MTV': 470,\n",
       "             'Noah': 471,\n",
       "             'Play': 472,\n",
       "             'UN': 473,\n",
       "             'again': 474,\n",
       "             'dies': 475,\n",
       "             'maker': 476,\n",
       "             'office': 477,\n",
       "             'plans': 478,\n",
       "             'raises': 479,\n",
       "             'vs': 480,\n",
       "             '20': 481,\n",
       "             'Bad': 482,\n",
       "             'Beats': 483,\n",
       "             'Clooney': 484,\n",
       "             'Demand': 485,\n",
       "             'Depp': 486,\n",
       "             'During': 487,\n",
       "             'India': 488,\n",
       "             'Kids': 489,\n",
       "             'LWren': 490,\n",
       "             'Like': 491,\n",
       "             'Pharrell': 492,\n",
       "             'Rates': 493,\n",
       "             'SNAPSHOTWall': 494,\n",
       "             'Shia': 495,\n",
       "             'Voice': 496,\n",
       "             'loss': 497,\n",
       "             'one': 498,\n",
       "             'study': 499,\n",
       "             '12': 500,\n",
       "             '50': 501,\n",
       "             'CORRECTEDUPDATE': 502,\n",
       "             'Fight': 503,\n",
       "             'Lana': 504,\n",
       "             'Mad': 505,\n",
       "             'Nick': 506,\n",
       "             'Reveals': 507,\n",
       "             'She': 508,\n",
       "             'Should': 509,\n",
       "             'Target': 510,\n",
       "             'box': 511,\n",
       "             'concerns': 512,\n",
       "             'dress': 513,\n",
       "             'drop': 514,\n",
       "             'global': 515,\n",
       "             'holds': 516,\n",
       "             'risk': 517,\n",
       "             'this': 518,\n",
       "             'Allergan': 519,\n",
       "             'Amazing': 520,\n",
       "             'Andrew': 521,\n",
       "             'Barclays': 522,\n",
       "             'Bell': 523,\n",
       "             'Bid': 524,\n",
       "             'Cast': 525,\n",
       "             'Del': 526,\n",
       "             'Do': 527,\n",
       "             'Instagram': 528,\n",
       "             'July': 529,\n",
       "             'Kunis': 530,\n",
       "             'Mila': 531,\n",
       "             'Plans': 532,\n",
       "             'Role': 533,\n",
       "             'Sex': 534,\n",
       "             'Simpson': 535,\n",
       "             'Swift': 536,\n",
       "             'Us': 537,\n",
       "             'View': 538,\n",
       "             'When': 539,\n",
       "             'boost': 540,\n",
       "             'debut': 541,\n",
       "             'eyes': 542,\n",
       "             'outlook': 543,\n",
       "             'plan': 544,\n",
       "             'probe': 545,\n",
       "             'rally': 546,\n",
       "             'trading': 547,\n",
       "             'weak': 548,\n",
       "             'Again': 549,\n",
       "             'Argentina': 550,\n",
       "             'Cuts': 551,\n",
       "             'Even': 552,\n",
       "             'Global': 553,\n",
       "             'Hollywood': 554,\n",
       "             'Kardashians': 555,\n",
       "             'Lady': 556,\n",
       "             'Makes': 557,\n",
       "             'Prices': 558,\n",
       "             'Red': 559,\n",
       "             'Stable': 560,\n",
       "             'Takes': 561,\n",
       "             'Tesla': 562,\n",
       "             'Things': 563,\n",
       "             'Way': 564,\n",
       "             'Wont': 565,\n",
       "             'baby': 566,\n",
       "             'bonds': 567,\n",
       "             'cars': 568,\n",
       "             'hike': 569,\n",
       "             'investors': 570,\n",
       "             'opens': 571,\n",
       "             'results': 572,\n",
       "             'wants': 573,\n",
       "             'Asia': 574,\n",
       "             'Captain': 575,\n",
       "             'Case': 576,\n",
       "             'Decline': 577,\n",
       "             'Family': 578,\n",
       "             'Gala': 579,\n",
       "             'Kristen': 580,\n",
       "             'Lea': 581,\n",
       "             'Live': 582,\n",
       "             'Take': 583,\n",
       "             'Yen': 584,\n",
       "             'cancer': 585,\n",
       "             'help': 586,\n",
       "             'next': 587,\n",
       "             'warns': 588,\n",
       "             'your': 589,\n",
       "             '13': 590,\n",
       "             'ATT': 591,\n",
       "             'Another': 592,\n",
       "             'Arrested': 593,\n",
       "             'BOJ': 594,\n",
       "             'Cut': 595,\n",
       "             'Did': 596,\n",
       "             'Former': 597,\n",
       "             'Franco': 598,\n",
       "             'London': 599,\n",
       "             'Look': 600,\n",
       "             'Man': 601,\n",
       "             'Photo': 602,\n",
       "             'Rally': 603,\n",
       "             'Rolling': 604,\n",
       "             'Three': 605,\n",
       "             'VII': 606,\n",
       "             'Wants': 607,\n",
       "             'being': 608,\n",
       "             'death': 609,\n",
       "             'during': 610,\n",
       "             'fans': 611,\n",
       "             'found': 612,\n",
       "             'merger': 613,\n",
       "             'sell': 614,\n",
       "             'sex': 615,\n",
       "             'video': 616,\n",
       "             '9': 617,\n",
       "             'Album': 618,\n",
       "             'Australia': 619,\n",
       "             'Being': 620,\n",
       "             'Biggest': 621,\n",
       "             'Director': 622,\n",
       "             'Garfield': 623,\n",
       "             'King': 624,\n",
       "             'Lena': 625,\n",
       "             'McDonalds': 626,\n",
       "             'Miranda': 627,\n",
       "             'Months': 628,\n",
       "             'Nasdaq': 629,\n",
       "             'Neil': 630,\n",
       "             'Paribas': 631,\n",
       "             'Rey': 632,\n",
       "             'Stanley': 633,\n",
       "             'VIDEO': 634,\n",
       "             'Virus': 635,\n",
       "             'Women': 636,\n",
       "             'Year': 637,\n",
       "             'demand': 638,\n",
       "             'estimates': 639,\n",
       "             'face': 640,\n",
       "             'forecast': 641,\n",
       "             'gain': 642,\n",
       "             'if': 643,\n",
       "             'under': 644,\n",
       "             'wins': 645,\n",
       "             'yen': 646,\n",
       "             '8': 647,\n",
       "             'Bill': 648,\n",
       "             'Black': 649,\n",
       "             'Business': 650,\n",
       "             'Christina': 651,\n",
       "             'Face': 652,\n",
       "             'Five': 653,\n",
       "             'Jobs': 654,\n",
       "             'Jr': 655,\n",
       "             'Jump': 656,\n",
       "             'Loss': 657,\n",
       "             'Mars': 658,\n",
       "             'Mother': 659,\n",
       "             'Mothers': 660,\n",
       "             'Near': 661,\n",
       "             'Probe': 662,\n",
       "             'Really': 663,\n",
       "             'Russell': 664,\n",
       "             'Seeks': 665,\n",
       "             'TMobile': 666,\n",
       "             'Too': 667,\n",
       "             'beats': 668,\n",
       "             'car': 669,\n",
       "             'close': 670,\n",
       "             'despite': 671,\n",
       "             'jumps': 672,\n",
       "             'lows': 673,\n",
       "             'or': 674,\n",
       "             'red': 675,\n",
       "             'supply': 676,\n",
       "             'tour': 677,\n",
       "             'Apparel': 678,\n",
       "             'Cars': 679,\n",
       "             'Cover': 680,\n",
       "             'Drug': 681,\n",
       "             'FCC': 682,\n",
       "             'GE': 683,\n",
       "             'Gas': 684,\n",
       "             'Go': 685,\n",
       "             'Im': 686,\n",
       "             'Opens': 687,\n",
       "             'Paris': 688,\n",
       "             'Pound': 689,\n",
       "             'RPTFitch': 690,\n",
       "             'Smith': 691,\n",
       "             'Times': 692,\n",
       "             'WSJ': 693,\n",
       "             'Watch': 694,\n",
       "             'fears': 695,\n",
       "             'goes': 696,\n",
       "             'life': 697,\n",
       "             'recall': 698,\n",
       "             'steps': 699,\n",
       "             'test': 700,\n",
       "             'while': 701,\n",
       "             'Ahead': 702,\n",
       "             'Child': 703,\n",
       "             'Claims': 704,\n",
       "             'Dunham': 705,\n",
       "             'ECBs': 706,\n",
       "             'GDP': 707,\n",
       "             'Need': 708,\n",
       "             'Posts': 709,\n",
       "             'Rihanna': 710,\n",
       "             'Rolf': 711,\n",
       "             'Rules': 712,\n",
       "             'Space': 713,\n",
       "             'Stones': 714,\n",
       "             'Tv': 715,\n",
       "             'Valeant': 716,\n",
       "             'Yahoo': 717,\n",
       "             'below': 718,\n",
       "             'edge': 719,\n",
       "             'film': 720,\n",
       "             'heart': 721,\n",
       "             'little': 722,\n",
       "             'posts': 723,\n",
       "             'unit': 724,\n",
       "             'Airlines': 725,\n",
       "             'Bets': 726,\n",
       "             'Brent': 727,\n",
       "             'British': 728,\n",
       "             'Bryan': 729,\n",
       "             'Comcast': 730,\n",
       "             'Dancing': 731,\n",
       "             'Finds': 732,\n",
       "             'Gaga': 733,\n",
       "             'Girls': 734,\n",
       "             'Glass': 735,\n",
       "             'Heart': 736,\n",
       "             'Higher': 737,\n",
       "             'Increase': 738,\n",
       "             'Katie': 739,\n",
       "             'LA': 740,\n",
       "             'Last': 741,\n",
       "             'Lopez': 742,\n",
       "             'Michele': 743,\n",
       "             'Michelle': 744,\n",
       "             'Money': 745,\n",
       "             'Party': 746,\n",
       "             'Policy': 747,\n",
       "             'Recap': 748,\n",
       "             'Rogen': 749,\n",
       "             'Say': 750,\n",
       "             'Short': 751,\n",
       "             'Solange': 752,\n",
       "             'Suisse': 753,\n",
       "             'Tax': 754,\n",
       "             'Treasury': 755,\n",
       "             'Were': 756,\n",
       "             'Woman': 757,\n",
       "             'amid': 758,\n",
       "             'away': 759,\n",
       "             'big': 760,\n",
       "             'crude': 761,\n",
       "             'dip': 762,\n",
       "             'economic': 763,\n",
       "             'edges': 764,\n",
       "             'party': 765,\n",
       "             'people': 766,\n",
       "             'possible': 767,\n",
       "             'rules': 768,\n",
       "             'second': 769,\n",
       "             'Americans': 770,\n",
       "             'Behind': 771,\n",
       "             'Bond': 772,\n",
       "             'Broadway': 773,\n",
       "             'De': 774,\n",
       "             'Harry': 775,\n",
       "             'IMF': 776,\n",
       "             'Ice': 777,\n",
       "             'Investors': 778,\n",
       "             'Jimmy': 779,\n",
       "             'McCarthy': 780,\n",
       "             'Or': 781,\n",
       "             'Our': 782,\n",
       "             'Recall': 783,\n",
       "             'See': 784,\n",
       "             'Shire': 785,\n",
       "             'Singer': 786,\n",
       "             'SpiderMan': 787,\n",
       "             'Tops': 788,\n",
       "             'Walker': 789,\n",
       "             'black': 790,\n",
       "             'daughter': 791,\n",
       "             'fight': 792,\n",
       "             'lawsuit': 793,\n",
       "             'nearly': 794,\n",
       "             'quarterly': 795,\n",
       "             'sale': 796,\n",
       "             'sources': 797,\n",
       "             'stake': 798,\n",
       "             '25': 799,\n",
       "             'Africa': 800,\n",
       "             'Asian': 801,\n",
       "             'Bachelorette': 802,\n",
       "             'Car': 803,\n",
       "             'Critics': 804,\n",
       "             'Debt': 805,\n",
       "             'Demi': 806,\n",
       "             'Disney': 807,\n",
       "             'Food': 808,\n",
       "             'France': 809,\n",
       "             'Jenny': 810,\n",
       "             'Kendall': 811,\n",
       "             'Lawsuit': 812,\n",
       "             'List': 813,\n",
       "             'Looks': 814,\n",
       "             'NY': 815,\n",
       "             'Next': 816,\n",
       "             'Offer': 817,\n",
       "             'Peaches': 818,\n",
       "             'Premiere': 819,\n",
       "             'Prince': 820,\n",
       "             'Push': 821,\n",
       "             'Raises': 822,\n",
       "             'Stock': 823,\n",
       "             'Transformers': 824,\n",
       "             'Tribute': 825,\n",
       "             'Wife': 826,\n",
       "             'Winter': 827,\n",
       "             'capital': 828,\n",
       "             'chief': 829,\n",
       "             'early': 830,\n",
       "             'easing': 831,\n",
       "             'expected': 832,\n",
       "             'family': 833,\n",
       "             'had': 834,\n",
       "             'health': 835,\n",
       "             'meeting': 836,\n",
       "             'names': 837,\n",
       "             'now': 838,\n",
       "             'plane': 839,\n",
       "             'raise': 840,\n",
       "             'settle': 841,\n",
       "             'some': 842,\n",
       "             'virus': 843,\n",
       "             'world': 844,\n",
       "             '11': 845,\n",
       "             '18': 846,\n",
       "             '3US': 847,\n",
       "             'Ben': 848,\n",
       "             'Better': 849,\n",
       "             'Birthday': 850,\n",
       "             'Cable': 851,\n",
       "             'Challenge': 852,\n",
       "             'Chrysler': 853,\n",
       "             'Coachella': 854,\n",
       "             'Company': 855,\n",
       "             'Days': 856,\n",
       "             'DirecTV': 857,\n",
       "             'Earnings': 858,\n",
       "             'Gay': 859,\n",
       "             'Johnson': 860,\n",
       "             'Joins': 861,\n",
       "             'Little': 862,\n",
       "             'Merger': 863,\n",
       "             'NEW': 864,\n",
       "             'Other': 865,\n",
       "             'President': 866,\n",
       "             'Q1': 867,\n",
       "             'Return': 868,\n",
       "             'Scotts': 869,\n",
       "             'Sequel': 870,\n",
       "             'Story': 871,\n",
       "             'Tech': 872,\n",
       "             'Test': 873,\n",
       "             'Their': 874,\n",
       "             'above': 875,\n",
       "             'action': 876,\n",
       "             'auto': 877,\n",
       "             'best': 878,\n",
       "             'costs': 879,\n",
       "             'crisis': 880,\n",
       "             'dips': 881,\n",
       "             'files': 882,\n",
       "             'hopes': 883,\n",
       "             'key': 884,\n",
       "             'leaves': 885,\n",
       "             'losses': 886,\n",
       "             'quarter': 887,\n",
       "             'start': 888,\n",
       "             '24': 889,\n",
       "             'Actor': 890,\n",
       "             'Advance': 891,\n",
       "             'Aereo': 892,\n",
       "             'Age': 893,\n",
       "             'California': 894,\n",
       "             'Carney': 895,\n",
       "             'Climbs': 896,\n",
       "             'Full': 897,\n",
       "             'Futures': 898,\n",
       "             'Galaxy': 899,\n",
       "             'HBO': 900,\n",
       "             'Had': 901,\n",
       "             'Help': 902,\n",
       "             'Hemsworth': 903,\n",
       "             'Kourtney': 904,\n",
       "             'Libya': 905,\n",
       "             'Lovato': 906,\n",
       "             'Lupita': 907,\n",
       "             'MA': 908,\n",
       "             'Making': 909,\n",
       "             'Mark': 910,\n",
       "             'Net': 911,\n",
       "             'Nikkei': 912,\n",
       "             'Nyongo': 913,\n",
       "             'PMI': 914,\n",
       "             'Rob': 915,\n",
       "             'Russian': 916,\n",
       "             'Scientists': 917,\n",
       "             'Shailene': 918,\n",
       "             'Shes': 919,\n",
       "             'Sir': 920,\n",
       "             'Split': 921,\n",
       "             'They': 922,\n",
       "             'Toyota': 923,\n",
       "             'Under': 924,\n",
       "             'Want': 925,\n",
       "             'Washington': 926,\n",
       "             'approves': 927,\n",
       "             'call': 928,\n",
       "             'company': 929,\n",
       "             'consumer': 930,\n",
       "             'gold': 931,\n",
       "             'highs': 932,\n",
       "             'man': 933,\n",
       "             'mortgage': 934,\n",
       "             'premiere': 935,\n",
       "             'price': 936,\n",
       "             'recalls': 937,\n",
       "             'reports': 938,\n",
       "             'sanctions': 939,\n",
       "             'say': 940,\n",
       "             'sells': 941,\n",
       "             'tops': 942,\n",
       "             'use': 943,\n",
       "             'Action': 944,\n",
       "             'Angeles': 945,\n",
       "             'Boeing': 946,\n",
       "             'CANADA': 947,\n",
       "             'Capital': 948,\n",
       "             'Downey': 949,\n",
       "             'Economic': 950,\n",
       "             'Everything': 951,\n",
       "             'Factors': 952,\n",
       "             'Finale': 953,\n",
       "             'Finally': 954,\n",
       "             'General': 955,\n",
       "             'Getting': 956,\n",
       "             'Goes': 957,\n",
       "             'Got': 958,\n",
       "             'Here': 959,\n",
       "             'Heres': 960,\n",
       "             'Jon': 961,\n",
       "             'LaBeouf': 962,\n",
       "             'Leads': 963,\n",
       "             'Lewis': 964,\n",
       "             'Los': 965,\n",
       "             'Malaysia': 966,\n",
       "             'Month': 967,\n",
       "             'Pattinson': 968,\n",
       "             'Perry': 969,\n",
       "             'Photos': 970,\n",
       "             'Power': 971,\n",
       "             'Senate': 972,\n",
       "             'Stimulus': 973,\n",
       "             'Swiss': 974,\n",
       "             'These': 975,\n",
       "             'Treasuries': 976,\n",
       "             'Vogue': 977,\n",
       "             'War': 978,\n",
       "             'YORK': 979,\n",
       "             'app': 980,\n",
       "             'biggest': 981,\n",
       "             'birthday': 982,\n",
       "             'confirms': 983,\n",
       "             'days': 984,\n",
       "             'dead': 985,\n",
       "             'firms': 986,\n",
       "             'focus': 987,\n",
       "             'get': 988,\n",
       "             'girl': 989,\n",
       "             'gives': 990,\n",
       "             'gown': 991,\n",
       "             'joins': 992,\n",
       "             'judge': 993,\n",
       "             'last': 994,\n",
       "             'lifts': 995,\n",
       "             'light': 996,\n",
       "             'look': 997,\n",
       "             'love': 998,\n",
       "             'month': 999,\n",
       "             'service': 1000,\n",
       "             ...})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(f'[{punctuation}\\n]')\n",
    "df = pl.concat([train_df, valid_df, test_df])\n",
    "title = df['title'].to_list()\n",
    "labels = df['category'].to_numpy()\n",
    "title = [i.split() for i in title]\n",
    "words = list(chain.from_iterable(title))\n",
    "words = [pattern.sub('', t) for t in words]\n",
    "words = [t for t in words if t]\n",
    "words = list(dict(Counter(words)).items())\n",
    "words.sort(key=lambda x: x[0])\n",
    "words.sort(key=lambda x: x[1], reverse=True)\n",
    "rank = defaultdict(int)\n",
    "\n",
    "i = 1\n",
    "for k, v in words:\n",
    "  if v >= 2:\n",
    "    rank[k] = i\n",
    "    i += 1\n",
    "  else:\n",
    "    rank[k] = 0\n",
    "\n",
    "rank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_map = {\n",
    "  'b': 0,\n",
    "  't': 1,\n",
    "  'e': 2,\n",
    "  'm': 3,\n",
    "}\n",
    "\n",
    "def convert(text: str):\n",
    "  return torch.tensor([rank[i] for i in text])\n",
    "\n",
    "X = title[:]\n",
    "X_ = [convert(t) for t in X]\n",
    "X_ = nn.utils.rnn.pad_sequence(X_, batch_first=True)\n",
    "\n",
    "y = [category_map[i] for i in labels]\n",
    "y = torch.tensor(y)\n",
    "\n",
    "X_train, X_valtest = X_[:len(train)], X_[len(train):]\n",
    "X_valid, X_test = X_[:len(valid)], X_[len(valid):]\n",
    "y_train, y_valtest = y[:len(train)], y[len(train):]\n",
    "y_valid, y_test = y[:len(valid)], y[len(valid):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "  def __init__(self, V, dw, dh, out_features, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.embed = nn.Embedding(V, dw, padding_idx=V-1)\n",
    "    self.rnn = nn.RNN(dw, dh, batch_first=True, nonlinearity='relu')\n",
    "    self.fc1 = nn.Linear(dh, out_features)\n",
    "    nn.init.kaiming_normal_(self.rnn.weight_ih_l0)\n",
    "    nn.init.kaiming_normal_(self.rnn.weight_hh_l0)\n",
    "    nn.init.kaiming_normal_(self.fc1.weight)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embed(x)\n",
    "    x, _ = self.rnn(x)\n",
    "    x = self.fc1(x[:, -1, :])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.13562336564064026\n"
     ]
    }
   ],
   "source": [
    "V = len(rank)+1\n",
    "dw = 300\n",
    "dh = 50\n",
    "out_features = 4\n",
    "model = RNN(V, dw, dh, out_features)\n",
    "y_pred = model(X_train)\n",
    "y_pred = torch.argmax(torch.softmax(y_pred, dim=1), axis=1)\n",
    "print(f'accuracy: {sum(y_train==y_pred)/len(y_train)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "  def __init__(self, X, y) -> None:\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return (self.X[idx], self.y[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(NewsDataset(X_train, y_train), shuffle=True)\n",
    "dataloader_valid = DataLoader(NewsDataset(X_valid, y_valid))\n",
    "dataloader_test = DataLoader(NewsDataset(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.177050, valid loss: 1.149136, train acc: 0.440565, valid acc: 0.493263\n",
      "train loss: 1.145101, valid loss: 1.154696, train acc: 0.488487, valid acc: 0.492515\n",
      "train loss: 1.125455, valid loss: 1.110760, train acc: 0.514695, valid acc: 0.538922\n",
      "train loss: 1.098434, valid loss: 1.047414, train acc: 0.549607, valid acc: 0.596557\n",
      "train loss: 1.057028, valid loss: 0.962059, train acc: 0.590603, valid acc: 0.649701\n",
      "train loss: 0.975652, valid loss: 0.882369, train acc: 0.649101, valid acc: 0.692365\n",
      "train loss: 0.908520, valid loss: 0.826739, train acc: 0.685324, valid acc: 0.711826\n",
      "train loss: 0.850112, valid loss: 0.784706, train acc: 0.712654, valid acc: 0.742515\n",
      "train loss: 0.805069, valid loss: 0.818617, train acc: 0.729783, valid acc: 0.735778\n",
      "train loss: 0.776296, valid loss: 0.702587, train acc: 0.736241, valid acc: 0.762725\n"
     ]
    }
   ],
   "source": [
    "model = RNN(V, dw, dh, out_features)\n",
    "optimizer = optim.SGD(params=model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "  model.train()\n",
    "  loss_train = 0\n",
    "  correct = 0\n",
    "  for X, y in dataloader_train:\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X)\n",
    "\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.softmax(y_pred, dim=1)\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    correct += torch.sum(y_pred == y).item()\n",
    "\n",
    "    loss_train += loss.item()\n",
    "\n",
    "  loss_train /= len(dataloader_train.dataset)\n",
    "  acc_train = correct / len(dataloader_train.dataset)\n",
    "\n",
    "  model.eval()\n",
    "  loss_valid = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader_valid:\n",
    "      y_pred = model(X)\n",
    "      loss = criterion(y_pred, y)\n",
    "\n",
    "      y_pred = torch.softmax(y_pred, dim=1)\n",
    "      y_pred = y_pred.argmax(dim=1)\n",
    "      correct += torch.sum(y_pred == y).item()\n",
    "\n",
    "      loss_valid += loss.item()*X.size(0)\n",
    "\n",
    "  loss_valid /= len(dataloader_valid.dataset)\n",
    "  acc_valid = correct / len(dataloader_valid.dataset)\n",
    "\n",
    "  print(f\"train loss: {loss_train:>7f}, valid loss: {loss_valid:>7f}, train acc: {acc_train:>7f}, valid acc: {acc_valid:>7f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(NewsDataset(X_train.to('cuda'), y_train.to('cuda')), shuffle=True, batch_size=64)\n",
    "dataloader_valid = DataLoader(NewsDataset(X_valid.to('cuda'), y_valid.to('cuda')), batch_size=64)\n",
    "dataloader_test = DataLoader(NewsDataset(X_test.to('cuda'), y_test.to('cuda')), batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.227665, valid loss: 1.159218, train acc: 0.426526, valid acc: 0.443862\n",
      "train loss: 1.161343, valid loss: 1.155567, train acc: 0.451329, valid acc: 0.455838\n",
      "train loss: 1.156402, valid loss: 1.149515, train acc: 0.460970, valid acc: 0.478293\n",
      "train loss: 1.153096, valid loss: 1.145550, train acc: 0.471640, valid acc: 0.494012\n",
      "train loss: 1.148491, valid loss: 1.143850, train acc: 0.481187, valid acc: 0.488024\n",
      "train loss: 1.146056, valid loss: 1.145004, train acc: 0.483433, valid acc: 0.490269\n",
      "train loss: 1.143856, valid loss: 1.139306, train acc: 0.490827, valid acc: 0.499251\n",
      "train loss: 1.142033, valid loss: 1.143545, train acc: 0.489985, valid acc: 0.489521\n",
      "train loss: 1.140609, valid loss: 1.137088, train acc: 0.492980, valid acc: 0.501497\n",
      "train loss: 1.138735, valid loss: 1.136735, train acc: 0.494478, valid acc: 0.505240\n"
     ]
    }
   ],
   "source": [
    "model = RNN(V, dw, dh, out_features).to('cuda')\n",
    "optimizer = optim.SGD(params=model.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "\n",
    "for epoch in range(10):\n",
    "  model.train()\n",
    "  loss_train = 0\n",
    "  correct = 0\n",
    "  for X, y in dataloader_train:\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X)\n",
    "\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.softmax(y_pred, dim=1)\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    correct += torch.sum(y_pred == y).item()\n",
    "\n",
    "    loss_train += loss.item()*X.size(0)\n",
    "\n",
    "  loss_train /= len(dataloader_train.dataset)\n",
    "  acc_train = correct / len(dataloader_train.dataset)\n",
    "\n",
    "  model.eval()\n",
    "  loss_valid = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader_valid:\n",
    "      y_pred = model(X)\n",
    "      loss = criterion(y_pred, y)\n",
    "\n",
    "      y_pred = torch.softmax(y_pred, dim=1)\n",
    "      y_pred = y_pred.argmax(dim=1)\n",
    "      correct += torch.sum(y_pred == y).item()\n",
    "\n",
    "      loss_valid += loss.item()*X.size(0)\n",
    "\n",
    "  loss_valid /= len(dataloader_valid.dataset)\n",
    "  acc_valid = correct / len(dataloader_valid.dataset)\n",
    "\n",
    "  print(f\"train loss: {loss_train:>7f}, valid loss: {loss_valid:>7f}, train acc: {acc_train:>7f}, valid acc: {acc_valid:>7f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen: Vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "  def __init__(self, V, dw, dh, out_features, weight=None, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    if weight is not None:\n",
    "      self.embed = nn.Embedding.from_pretrained(weight, freeze=False, padding_idx=V-1)\n",
    "    else:\n",
    "      self.embed = nn.Embedding(V, dw, padding_idx=V-1)\n",
    "    self.rnn = nn.RNN(dw, dh, batch_first=True, nonlinearity='relu')\n",
    "    self.fc1 = nn.Linear(dh, out_features)\n",
    "    nn.init.kaiming_normal_(self.rnn.weight_ih_l0)\n",
    "    nn.init.kaiming_normal_(self.rnn.weight_hh_l0)\n",
    "    nn.init.kaiming_normal_(self.fc1.weight)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embed(x)\n",
    "    x, _ = self.rnn(x)\n",
    "    x = self.fc1(x[:, -1, :])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.zeros(V+1, dw).to('cuda')\n",
    "\n",
    "for k, v in rank.items():\n",
    "  if v == 0:\n",
    "    continue\n",
    "\n",
    "  try:\n",
    "    weight[v] = torch.tensor(gen[k]).to('cuda')\n",
    "  except:\n",
    "    weight[v] = torch.randn(dw).to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.418830, valid loss: 1.309550, train acc: 0.291838, valid acc: 0.385479\n",
      "train loss: 1.284337, valid loss: 1.264725, train acc: 0.401722, valid acc: 0.417665\n",
      "train loss: 1.246413, valid loss: 1.235283, train acc: 0.441595, valid acc: 0.431886\n",
      "train loss: 1.220367, valid loss: 1.212662, train acc: 0.443748, valid acc: 0.437126\n",
      "train loss: 1.199037, valid loss: 1.193046, train acc: 0.441969, valid acc: 0.428892\n",
      "train loss: 1.181754, valid loss: 1.177773, train acc: 0.445994, valid acc: 0.436377\n",
      "train loss: 1.169650, valid loss: 1.166854, train acc: 0.444684, valid acc: 0.431886\n",
      "train loss: 1.162125, valid loss: 1.159914, train acc: 0.447304, valid acc: 0.438623\n",
      "train loss: 1.158211, valid loss: 1.155756, train acc: 0.454137, valid acc: 0.447605\n",
      "train loss: 1.155773, valid loss: 1.152606, train acc: 0.455541, valid acc: 0.453593\n",
      "train loss: 1.153944, valid loss: 1.150444, train acc: 0.460876, valid acc: 0.455090\n",
      "train loss: 1.152468, valid loss: 1.148561, train acc: 0.462561, valid acc: 0.466317\n",
      "train loss: 1.151243, valid loss: 1.147153, train acc: 0.469113, valid acc: 0.467814\n",
      "train loss: 1.150098, valid loss: 1.145944, train acc: 0.473512, valid acc: 0.472305\n",
      "train loss: 1.148917, valid loss: 1.144318, train acc: 0.476413, valid acc: 0.480539\n",
      "train loss: 1.147550, valid loss: 1.142455, train acc: 0.485492, valid acc: 0.494760\n",
      "train loss: 1.145724, valid loss: 1.140148, train acc: 0.497847, valid acc: 0.504491\n",
      "train loss: 1.143174, valid loss: 1.136225, train acc: 0.512636, valid acc: 0.531437\n",
      "train loss: 1.139423, valid loss: 1.130557, train acc: 0.530045, valid acc: 0.555389\n",
      "train loss: 1.134236, valid loss: 1.121895, train acc: 0.545114, valid acc: 0.575599\n",
      "train loss: 1.123716, valid loss: 1.106430, train acc: 0.562055, valid acc: 0.602545\n",
      "train loss: 1.098045, valid loss: 1.065212, train acc: 0.581524, valid acc: 0.625000\n",
      "train loss: 1.045163, valid loss: 0.993240, train acc: 0.619150, valid acc: 0.660928\n",
      "train loss: 0.975880, valid loss: 0.921097, train acc: 0.661924, valid acc: 0.689371\n",
      "train loss: 0.912825, valid loss: 0.864299, train acc: 0.690565, valid acc: 0.719311\n",
      "train loss: 0.861389, valid loss: 0.817947, train acc: 0.712374, valid acc: 0.727545\n",
      "train loss: 0.818131, valid loss: 0.784805, train acc: 0.728472, valid acc: 0.732036\n",
      "train loss: 0.783893, valid loss: 0.740612, train acc: 0.736803, valid acc: 0.749251\n",
      "train loss: 0.755875, valid loss: 0.715061, train acc: 0.745975, valid acc: 0.754491\n",
      "train loss: 0.738361, valid loss: 0.698381, train acc: 0.751030, valid acc: 0.755988\n",
      "train loss: 0.723079, valid loss: 0.685314, train acc: 0.753650, valid acc: 0.758982\n",
      "train loss: 0.711440, valid loss: 0.671693, train acc: 0.757301, valid acc: 0.763473\n",
      "train loss: 0.699705, valid loss: 0.663168, train acc: 0.760109, valid acc: 0.764970\n",
      "train loss: 0.690825, valid loss: 0.651644, train acc: 0.762542, valid acc: 0.768713\n",
      "train loss: 0.682004, valid loss: 0.651729, train acc: 0.763759, valid acc: 0.766467\n",
      "train loss: 0.670867, valid loss: 0.640930, train acc: 0.767222, valid acc: 0.774701\n",
      "train loss: 0.665261, valid loss: 0.650144, train acc: 0.768252, valid acc: 0.770210\n",
      "train loss: 0.658639, valid loss: 0.625482, train acc: 0.771060, valid acc: 0.773204\n",
      "train loss: 0.652443, valid loss: 0.621141, train acc: 0.771340, valid acc: 0.778443\n",
      "train loss: 0.648596, valid loss: 0.615019, train acc: 0.772370, valid acc: 0.778443\n",
      "train loss: 0.642427, valid loss: 0.608975, train acc: 0.772276, valid acc: 0.777695\n",
      "train loss: 0.637706, valid loss: 0.608220, train acc: 0.773961, valid acc: 0.782186\n",
      "train loss: 0.635079, valid loss: 0.602635, train acc: 0.775552, valid acc: 0.779192\n",
      "train loss: 0.630900, valid loss: 0.615615, train acc: 0.775365, valid acc: 0.777695\n",
      "train loss: 0.627203, valid loss: 0.592961, train acc: 0.778267, valid acc: 0.778443\n",
      "train loss: 0.622658, valid loss: 0.601759, train acc: 0.777799, valid acc: 0.779940\n",
      "train loss: 0.616763, valid loss: 0.584948, train acc: 0.778267, valid acc: 0.779192\n",
      "train loss: 0.614357, valid loss: 0.580183, train acc: 0.777799, valid acc: 0.785928\n",
      "train loss: 0.612257, valid loss: 0.581175, train acc: 0.779577, valid acc: 0.784431\n",
      "train loss: 0.606550, valid loss: 0.616555, train acc: 0.780700, valid acc: 0.777695\n",
      "train loss: 0.605200, valid loss: 0.593186, train acc: 0.780232, valid acc: 0.781437\n",
      "train loss: 0.604005, valid loss: 0.574501, train acc: 0.780981, valid acc: 0.787425\n",
      "train loss: 0.595779, valid loss: 0.563794, train acc: 0.782946, valid acc: 0.788922\n",
      "train loss: 0.593878, valid loss: 0.561980, train acc: 0.782104, valid acc: 0.782934\n",
      "train loss: 0.591726, valid loss: 0.564317, train acc: 0.782853, valid acc: 0.787425\n",
      "train loss: 0.589289, valid loss: 0.568029, train acc: 0.784070, valid acc: 0.788174\n",
      "train loss: 0.587224, valid loss: 0.566120, train acc: 0.784725, valid acc: 0.783683\n",
      "train loss: 0.583657, valid loss: 0.553958, train acc: 0.786129, valid acc: 0.790419\n",
      "train loss: 0.581730, valid loss: 0.561460, train acc: 0.786597, valid acc: 0.783683\n",
      "train loss: 0.579533, valid loss: 0.563192, train acc: 0.786222, valid acc: 0.782186\n",
      "train loss: 0.577634, valid loss: 0.546200, train acc: 0.786878, valid acc: 0.794910\n",
      "train loss: 0.574856, valid loss: 0.544672, train acc: 0.784818, valid acc: 0.791168\n",
      "train loss: 0.571685, valid loss: 0.535580, train acc: 0.788001, valid acc: 0.796407\n",
      "train loss: 0.572651, valid loss: 0.537638, train acc: 0.786690, valid acc: 0.793413\n",
      "train loss: 0.567540, valid loss: 0.584367, train acc: 0.790341, valid acc: 0.782186\n",
      "train loss: 0.566825, valid loss: 0.533377, train acc: 0.792213, valid acc: 0.797156\n",
      "train loss: 0.563636, valid loss: 0.532799, train acc: 0.790434, valid acc: 0.793413\n",
      "train loss: 0.563155, valid loss: 0.550016, train acc: 0.791277, valid acc: 0.794910\n",
      "train loss: 0.557672, valid loss: 0.526267, train acc: 0.789498, valid acc: 0.796407\n",
      "train loss: 0.554308, valid loss: 0.601639, train acc: 0.793804, valid acc: 0.776946\n",
      "train loss: 0.554964, valid loss: 0.529698, train acc: 0.794553, valid acc: 0.795659\n",
      "train loss: 0.550209, valid loss: 0.554528, train acc: 0.794459, valid acc: 0.789671\n",
      "train loss: 0.551334, valid loss: 0.523324, train acc: 0.793336, valid acc: 0.794910\n",
      "train loss: 0.545922, valid loss: 0.593225, train acc: 0.796144, valid acc: 0.783683\n",
      "train loss: 0.547119, valid loss: 0.531011, train acc: 0.794553, valid acc: 0.801647\n",
      "train loss: 0.545704, valid loss: 0.512828, train acc: 0.796425, valid acc: 0.805389\n",
      "train loss: 0.540189, valid loss: 0.518691, train acc: 0.795395, valid acc: 0.800898\n",
      "train loss: 0.541136, valid loss: 0.514427, train acc: 0.795957, valid acc: 0.797156\n",
      "train loss: 0.538528, valid loss: 0.506695, train acc: 0.796799, valid acc: 0.806138\n",
      "train loss: 0.538911, valid loss: 0.507260, train acc: 0.796893, valid acc: 0.811377\n",
      "train loss: 0.537905, valid loss: 0.507196, train acc: 0.795957, valid acc: 0.805389\n",
      "train loss: 0.532317, valid loss: 0.506904, train acc: 0.798390, valid acc: 0.801647\n",
      "train loss: 0.534240, valid loss: 0.508531, train acc: 0.798390, valid acc: 0.804641\n",
      "train loss: 0.535849, valid loss: 0.577447, train acc: 0.794646, valid acc: 0.786677\n",
      "train loss: 0.529934, valid loss: 0.518310, train acc: 0.796893, valid acc: 0.806886\n",
      "train loss: 0.525540, valid loss: 0.502850, train acc: 0.798390, valid acc: 0.807635\n",
      "train loss: 0.528769, valid loss: 0.500709, train acc: 0.798297, valid acc: 0.806138\n",
      "train loss: 0.524144, valid loss: 0.501425, train acc: 0.798577, valid acc: 0.812874\n",
      "train loss: 0.522743, valid loss: 0.499263, train acc: 0.800356, valid acc: 0.806886\n",
      "train loss: 0.523589, valid loss: 0.496896, train acc: 0.800824, valid acc: 0.811377\n",
      "train loss: 0.521074, valid loss: 0.523959, train acc: 0.803257, valid acc: 0.797156\n",
      "train loss: 0.521335, valid loss: 0.499735, train acc: 0.803351, valid acc: 0.809880\n",
      "train loss: 0.516928, valid loss: 0.505470, train acc: 0.801479, valid acc: 0.804641\n",
      "train loss: 0.516949, valid loss: 0.509248, train acc: 0.801104, valid acc: 0.815868\n",
      "train loss: 0.514084, valid loss: 0.491096, train acc: 0.803070, valid acc: 0.810629\n",
      "train loss: 0.513984, valid loss: 0.489452, train acc: 0.801104, valid acc: 0.810629\n",
      "train loss: 0.508090, valid loss: 0.509383, train acc: 0.805504, valid acc: 0.802395\n",
      "train loss: 0.509743, valid loss: 0.483605, train acc: 0.804006, valid acc: 0.822605\n",
      "train loss: 0.505722, valid loss: 0.487340, train acc: 0.806252, valid acc: 0.811377\n",
      "train loss: 0.511100, valid loss: 0.494801, train acc: 0.801760, valid acc: 0.806886\n"
     ]
    }
   ],
   "source": [
    "model = RNN(V, dw, dh, out_features, weight).to('cuda')\n",
    "optimizer = optim.SGD(params=model.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "\n",
    "for epoch in range(100):\n",
    "  model.train()\n",
    "  loss_train = 0\n",
    "  correct = 0\n",
    "  for X, y in dataloader_train:\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X)\n",
    "\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.softmax(y_pred, dim=1)\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    correct += torch.sum(y_pred == y).item()\n",
    "\n",
    "    loss_train += loss.item()*X.size(0)\n",
    "\n",
    "  loss_train /= len(dataloader_train.dataset)\n",
    "  acc_train = correct / len(dataloader_train.dataset)\n",
    "\n",
    "  model.eval()\n",
    "  loss_valid = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader_valid:\n",
    "      y_pred = model(X)\n",
    "      loss = criterion(y_pred, y)\n",
    "\n",
    "      y_pred = torch.softmax(y_pred, dim=1)\n",
    "      y_pred = y_pred.argmax(dim=1)\n",
    "      correct += torch.sum(y_pred == y).item()\n",
    "\n",
    "      loss_valid += loss.item()*X.size(0)\n",
    "\n",
    "  loss_valid /= len(dataloader_valid.dataset)\n",
    "  acc_valid = correct / len(dataloader_valid.dataset)\n",
    "\n",
    "  print(f\"train loss: {loss_train:>7f}, valid loss: {loss_valid:>7f}, train acc: {acc_train:>7f}, valid acc: {acc_valid:>7f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "  def __init__(self, V, dw, dh, out_features, weight=None, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    if weight is not None:\n",
    "      self.embed = nn.Embedding.from_pretrained(weight, freeze=False, padding_idx=V-1)\n",
    "    else:\n",
    "      self.embed = nn.Embedding(V, dw, padding_idx=V-1)\n",
    "    self.rnn = nn.RNN(dw, dh, batch_first=True, nonlinearity='relu', bidirectional=True)\n",
    "    self.fc1 = nn.Linear(dh*2, out_features)\n",
    "    nn.init.kaiming_normal_(self.rnn.weight_ih_l0)\n",
    "    nn.init.kaiming_normal_(self.rnn.weight_hh_l0)\n",
    "    nn.init.kaiming_normal_(self.rnn.weight_ih_l0_reverse)\n",
    "    nn.init.kaiming_normal_(self.rnn.weight_hh_l0_reverse)\n",
    "    nn.init.kaiming_normal_(self.fc1.weight)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embed(x)\n",
    "    _, x = self.rnn(x)\n",
    "    rnn_out = torch.cat([x[-2,:,:], x[-1,:,:]], dim=1)\n",
    "    x = self.fc1(rnn_out)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.294980, valid loss: 1.191153, train acc: 0.364283, valid acc: 0.402695\n",
      "train loss: 1.176916, valid loss: 1.154146, train acc: 0.435230, valid acc: 0.454341\n",
      "train loss: 1.151639, valid loss: 1.137687, train acc: 0.466492, valid acc: 0.475299\n",
      "train loss: 1.135063, valid loss: 1.121828, train acc: 0.492980, valid acc: 0.498503\n",
      "train loss: 1.120134, valid loss: 1.107513, train acc: 0.515724, valid acc: 0.526946\n",
      "train loss: 1.105861, valid loss: 1.094146, train acc: 0.535848, valid acc: 0.550150\n",
      "train loss: 1.091950, valid loss: 1.079175, train acc: 0.554661, valid acc: 0.572605\n",
      "train loss: 1.077744, valid loss: 1.063068, train acc: 0.572070, valid acc: 0.592066\n",
      "train loss: 1.061610, valid loss: 1.045625, train acc: 0.590884, valid acc: 0.613772\n",
      "train loss: 1.041654, valid loss: 1.018753, train acc: 0.608574, valid acc: 0.633234\n",
      "train loss: 1.015277, valid loss: 0.981156, train acc: 0.627668, valid acc: 0.648204\n",
      "train loss: 0.974787, valid loss: 0.936469, train acc: 0.645638, valid acc: 0.654192\n",
      "train loss: 0.928940, valid loss: 0.886118, train acc: 0.665107, valid acc: 0.678144\n",
      "train loss: 0.880428, valid loss: 0.855475, train acc: 0.686634, valid acc: 0.689371\n",
      "train loss: 0.838966, valid loss: 0.802919, train acc: 0.704699, valid acc: 0.717066\n",
      "train loss: 0.804788, valid loss: 0.781705, train acc: 0.716398, valid acc: 0.728293\n",
      "train loss: 0.777434, valid loss: 0.731531, train acc: 0.725384, valid acc: 0.751497\n",
      "train loss: 0.757405, valid loss: 0.716313, train acc: 0.731468, valid acc: 0.754491\n",
      "train loss: 0.734452, valid loss: 0.731893, train acc: 0.738768, valid acc: 0.737275\n",
      "train loss: 0.715593, valid loss: 0.674972, train acc: 0.744384, valid acc: 0.770958\n",
      "train loss: 0.700066, valid loss: 0.657350, train acc: 0.747754, valid acc: 0.776198\n",
      "train loss: 0.685927, valid loss: 0.644592, train acc: 0.752246, valid acc: 0.776198\n",
      "train loss: 0.676085, valid loss: 0.649208, train acc: 0.755803, valid acc: 0.764222\n",
      "train loss: 0.662891, valid loss: 0.639513, train acc: 0.758517, valid acc: 0.765719\n",
      "train loss: 0.652305, valid loss: 0.616837, train acc: 0.761045, valid acc: 0.783683\n",
      "train loss: 0.644042, valid loss: 0.625603, train acc: 0.763665, valid acc: 0.776946\n",
      "train loss: 0.637840, valid loss: 0.597414, train acc: 0.765912, valid acc: 0.788174\n",
      "train loss: 0.627035, valid loss: 0.598407, train acc: 0.765069, valid acc: 0.782934\n",
      "train loss: 0.618518, valid loss: 0.607238, train acc: 0.769749, valid acc: 0.773952\n",
      "train loss: 0.610840, valid loss: 0.584164, train acc: 0.772183, valid acc: 0.786677\n",
      "train loss: 0.607018, valid loss: 0.571116, train acc: 0.772838, valid acc: 0.790419\n",
      "train loss: 0.598055, valid loss: 0.567848, train acc: 0.777705, valid acc: 0.791916\n",
      "train loss: 0.593236, valid loss: 0.585084, train acc: 0.779203, valid acc: 0.781437\n",
      "train loss: 0.586960, valid loss: 0.576729, train acc: 0.780607, valid acc: 0.779940\n",
      "train loss: 0.582922, valid loss: 0.551760, train acc: 0.780794, valid acc: 0.791168\n",
      "train loss: 0.577222, valid loss: 0.545872, train acc: 0.783227, valid acc: 0.788922\n",
      "train loss: 0.572157, valid loss: 0.542558, train acc: 0.784257, valid acc: 0.792665\n",
      "train loss: 0.566362, valid loss: 0.540450, train acc: 0.784725, valid acc: 0.797156\n",
      "train loss: 0.561571, valid loss: 0.534466, train acc: 0.788188, valid acc: 0.799401\n",
      "train loss: 0.555828, valid loss: 0.528662, train acc: 0.791370, valid acc: 0.799401\n",
      "train loss: 0.550528, valid loss: 0.524944, train acc: 0.790621, valid acc: 0.800898\n",
      "train loss: 0.545892, valid loss: 0.530662, train acc: 0.793336, valid acc: 0.801647\n",
      "train loss: 0.542584, valid loss: 0.525885, train acc: 0.793336, valid acc: 0.805389\n",
      "train loss: 0.538324, valid loss: 0.513138, train acc: 0.796050, valid acc: 0.806138\n",
      "train loss: 0.532030, valid loss: 0.507899, train acc: 0.795114, valid acc: 0.807635\n",
      "train loss: 0.530512, valid loss: 0.506675, train acc: 0.797829, valid acc: 0.805389\n",
      "train loss: 0.527236, valid loss: 0.507600, train acc: 0.797173, valid acc: 0.808383\n",
      "train loss: 0.521349, valid loss: 0.498965, train acc: 0.799139, valid acc: 0.806138\n",
      "train loss: 0.518297, valid loss: 0.494424, train acc: 0.801479, valid acc: 0.812126\n",
      "train loss: 0.513140, valid loss: 0.494916, train acc: 0.802883, valid acc: 0.818114\n",
      "train loss: 0.510706, valid loss: 0.519824, train acc: 0.803351, valid acc: 0.803144\n",
      "train loss: 0.505006, valid loss: 0.487448, train acc: 0.805223, valid acc: 0.814371\n",
      "train loss: 0.500492, valid loss: 0.481990, train acc: 0.808873, valid acc: 0.822605\n",
      "train loss: 0.498224, valid loss: 0.478199, train acc: 0.811868, valid acc: 0.821108\n",
      "train loss: 0.494457, valid loss: 0.490026, train acc: 0.812991, valid acc: 0.815868\n",
      "train loss: 0.489882, valid loss: 0.471580, train acc: 0.814395, valid acc: 0.827096\n",
      "train loss: 0.485284, valid loss: 0.470636, train acc: 0.816080, valid acc: 0.821108\n",
      "train loss: 0.483506, valid loss: 0.462610, train acc: 0.818982, valid acc: 0.830090\n",
      "train loss: 0.480505, valid loss: 0.479805, train acc: 0.819450, valid acc: 0.815120\n",
      "train loss: 0.475951, valid loss: 0.473233, train acc: 0.821790, valid acc: 0.817365\n",
      "train loss: 0.472978, valid loss: 0.458995, train acc: 0.821041, valid acc: 0.832335\n",
      "train loss: 0.468747, valid loss: 0.472755, train acc: 0.827312, valid acc: 0.822605\n",
      "train loss: 0.465902, valid loss: 0.449378, train acc: 0.826001, valid acc: 0.836078\n",
      "train loss: 0.461228, valid loss: 0.445642, train acc: 0.828435, valid acc: 0.836078\n",
      "train loss: 0.457613, valid loss: 0.474086, train acc: 0.829090, valid acc: 0.812126\n",
      "train loss: 0.454527, valid loss: 0.447031, train acc: 0.827967, valid acc: 0.830090\n",
      "train loss: 0.449859, valid loss: 0.431286, train acc: 0.831617, valid acc: 0.838323\n",
      "train loss: 0.447171, valid loss: 0.438217, train acc: 0.835829, valid acc: 0.830838\n",
      "train loss: 0.444189, valid loss: 0.436889, train acc: 0.834800, valid acc: 0.835329\n",
      "train loss: 0.440016, valid loss: 0.423658, train acc: 0.839105, valid acc: 0.844311\n",
      "train loss: 0.436489, valid loss: 0.430847, train acc: 0.837982, valid acc: 0.836078\n",
      "train loss: 0.433936, valid loss: 0.453938, train acc: 0.839667, valid acc: 0.827844\n",
      "train loss: 0.429748, valid loss: 0.423347, train acc: 0.840322, valid acc: 0.841317\n",
      "train loss: 0.426779, valid loss: 0.410320, train acc: 0.844347, valid acc: 0.852545\n",
      "train loss: 0.424665, valid loss: 0.416930, train acc: 0.842943, valid acc: 0.844311\n",
      "train loss: 0.421017, valid loss: 0.411750, train acc: 0.845657, valid acc: 0.845060\n",
      "train loss: 0.417673, valid loss: 0.411044, train acc: 0.845657, valid acc: 0.841317\n",
      "train loss: 0.415671, valid loss: 0.397794, train acc: 0.846499, valid acc: 0.858533\n",
      "train loss: 0.412555, valid loss: 0.399328, train acc: 0.847623, valid acc: 0.854042\n",
      "train loss: 0.406985, valid loss: 0.398114, train acc: 0.852209, valid acc: 0.848802\n",
      "train loss: 0.404418, valid loss: 0.426608, train acc: 0.850618, valid acc: 0.849551\n",
      "train loss: 0.403552, valid loss: 0.390688, train acc: 0.851647, valid acc: 0.860030\n",
      "train loss: 0.399465, valid loss: 0.384935, train acc: 0.852209, valid acc: 0.865269\n",
      "train loss: 0.395810, valid loss: 0.404307, train acc: 0.853894, valid acc: 0.852545\n",
      "train loss: 0.393688, valid loss: 0.384955, train acc: 0.855485, valid acc: 0.862275\n",
      "train loss: 0.390398, valid loss: 0.393057, train acc: 0.858106, valid acc: 0.856287\n",
      "train loss: 0.387407, valid loss: 0.380617, train acc: 0.859135, valid acc: 0.864521\n",
      "train loss: 0.384656, valid loss: 0.375302, train acc: 0.859603, valid acc: 0.869760\n",
      "train loss: 0.382442, valid loss: 0.373266, train acc: 0.860165, valid acc: 0.869012\n",
      "train loss: 0.380099, valid loss: 0.368093, train acc: 0.862411, valid acc: 0.871257\n",
      "train loss: 0.377480, valid loss: 0.364838, train acc: 0.863815, valid acc: 0.873503\n",
      "train loss: 0.375261, valid loss: 0.362947, train acc: 0.864283, valid acc: 0.872006\n",
      "train loss: 0.371765, valid loss: 0.359831, train acc: 0.864189, valid acc: 0.877994\n",
      "train loss: 0.368049, valid loss: 0.358694, train acc: 0.867933, valid acc: 0.872754\n",
      "train loss: 0.367036, valid loss: 0.366402, train acc: 0.868495, valid acc: 0.865269\n",
      "train loss: 0.362850, valid loss: 0.355027, train acc: 0.868401, valid acc: 0.878743\n",
      "train loss: 0.359881, valid loss: 0.367103, train acc: 0.872332, valid acc: 0.869760\n",
      "train loss: 0.357531, valid loss: 0.364647, train acc: 0.870741, valid acc: 0.877994\n",
      "train loss: 0.354591, valid loss: 0.356720, train acc: 0.873736, valid acc: 0.875000\n",
      "train loss: 0.353719, valid loss: 0.343468, train acc: 0.873549, valid acc: 0.883982\n"
     ]
    }
   ],
   "source": [
    "model = RNN(V, dw, dh, out_features, weight).to('cuda')\n",
    "optimizer = optim.SGD(params=model.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "\n",
    "for epoch in range(100):\n",
    "  model.train()\n",
    "  loss_train = 0\n",
    "  correct = 0\n",
    "  for X, y in dataloader_train:\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X)\n",
    "\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.softmax(y_pred, dim=1)\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    correct += torch.sum(y_pred == y).item()\n",
    "\n",
    "    loss_train += loss.item()*X.size(0)\n",
    "\n",
    "  loss_train /= len(dataloader_train.dataset)\n",
    "  acc_train = correct / len(dataloader_train.dataset)\n",
    "\n",
    "  model.eval()\n",
    "  loss_valid = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader_valid:\n",
    "      y_pred = model(X)\n",
    "      loss = criterion(y_pred, y)\n",
    "\n",
    "      y_pred = torch.softmax(y_pred, dim=1)\n",
    "      y_pred = y_pred.argmax(dim=1)\n",
    "      correct += torch.sum(y_pred == y).item()\n",
    "\n",
    "      loss_valid += loss.item()*X.size(0)\n",
    "\n",
    "  loss_valid /= len(dataloader_valid.dataset)\n",
    "  acc_valid = correct / len(dataloader_valid.dataset)\n",
    "\n",
    "  print(f\"train loss: {loss_train:>7f}, valid loss: {loss_valid:>7f}, train acc: {acc_train:>7f}, valid acc: {acc_valid:>7f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self, V, dw, dh, out_features, weight=None, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "    if weight is not None:\n",
    "      self.embed = nn.Embedding.from_pretrained(weight, freeze=False, padding_idx=V-1)\n",
    "    else:\n",
    "      self.embed = nn.Embedding(V, dw, padding_idx=V-1)\n",
    "    self.conv = nn.Conv2d(1, dh, (3, dw), padding=(1, 0))\n",
    "    self.relu = nn.ReLU()\n",
    "    # self.pool = nn.MaxPool1d(20)\n",
    "    self.fc1 = nn.Linear(dh, out_features)\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    x = self.embed(x)\n",
    "    x = x.unsqueeze(1)\n",
    "    x = self.conv(x).squeeze(3)\n",
    "    x = self.relu(x)\n",
    "    x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "    x = self.fc1(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1705,  0.2425,  0.0852,  0.5006],\n",
       "        [-0.0130,  0.1008, -0.1564,  0.3319],\n",
       "        [ 0.0647,  0.1928, -0.0469,  0.2969],\n",
       "        [ 0.0374,  0.1055, -0.0766,  0.2168],\n",
       "        [ 0.0760,  0.1008, -0.1489,  0.3028],\n",
       "        [ 0.1728,  0.2770,  0.1563,  0.5495],\n",
       "        [ 0.0044,  0.1348, -0.0107,  0.3237],\n",
       "        [-0.1548,  0.4377,  0.1412,  0.7650],\n",
       "        [-0.0468,  0.2362, -0.0852,  0.5560],\n",
       "        [ 0.0389,  0.0888, -0.0913,  0.3029],\n",
       "        [ 0.0856,  0.0453, -0.1358,  0.2373],\n",
       "        [ 0.0625,  0.0419, -0.1638,  0.1898],\n",
       "        [ 0.0566,  0.0712, -0.1270,  0.2444],\n",
       "        [ 0.0778,  0.1704,  0.1620,  0.5226],\n",
       "        [-0.2187,  0.5162,  0.0549,  0.8411],\n",
       "        [-0.0759,  0.2525, -0.0608,  0.5597],\n",
       "        [-0.0822,  0.5950,  0.1293,  0.9196],\n",
       "        [ 0.0850,  0.1614, -0.0778,  0.3432],\n",
       "        [-0.2561,  0.6025,  0.1409,  0.6821],\n",
       "        [ 0.0056,  0.0151, -0.1740,  0.2810],\n",
       "        [ 0.0099,  0.1329, -0.0802,  0.2953],\n",
       "        [ 0.0504,  0.0244, -0.1391,  0.3600],\n",
       "        [-0.1044,  0.4091,  0.1015,  0.7163],\n",
       "        [ 0.0696,  0.0383, -0.0532,  0.2745],\n",
       "        [ 0.0788,  0.0687, -0.0665,  0.2650],\n",
       "        [ 0.0649,  0.1347, -0.0970,  0.3978],\n",
       "        [-0.2387,  0.4357,  0.1359,  0.7310],\n",
       "        [ 0.0418,  0.1404, -0.0902,  0.3081],\n",
       "        [ 0.0261,  0.0570, -0.1048,  0.2391],\n",
       "        [ 0.0613,  0.1223, -0.1290,  0.3154],\n",
       "        [ 0.0722,  0.1183, -0.1695,  0.2672],\n",
       "        [ 0.0703,  0.0868, -0.1482,  0.2451],\n",
       "        [ 0.0402,  0.0789, -0.1010,  0.2492],\n",
       "        [-0.2666,  0.5591,  0.0903,  0.8528],\n",
       "        [-0.1192,  0.6538,  0.1174,  0.8868],\n",
       "        [-0.0280,  0.1420, -0.0790,  0.2664],\n",
       "        [-0.1336,  0.1233,  0.0061,  0.4661],\n",
       "        [-0.1583,  0.4389,  0.0851,  0.8110],\n",
       "        [-0.1545,  0.4925,  0.0807,  0.7603],\n",
       "        [-0.1218,  0.4550,  0.1749,  0.7465],\n",
       "        [ 0.0078,  0.0311, -0.1630,  0.2782],\n",
       "        [ 0.0265,  0.1222, -0.0733,  0.3211],\n",
       "        [ 0.0525,  0.1159, -0.1109,  0.2963],\n",
       "        [-0.0346,  0.3424,  0.1408,  0.7897],\n",
       "        [ 0.1145,  0.0229, -0.1579,  0.2433],\n",
       "        [-0.1490,  0.5057,  0.1295,  0.7489],\n",
       "        [ 0.0639,  0.1103, -0.0825,  0.2853],\n",
       "        [ 0.0663,  0.1660, -0.0717,  0.3045],\n",
       "        [ 0.0104,  0.1029, -0.1232,  0.2584],\n",
       "        [ 0.0281,  0.1346, -0.0454,  0.3802],\n",
       "        [-0.0233,  0.2659, -0.0673,  0.5783],\n",
       "        [-0.1568,  0.5567,  0.1617,  0.8286],\n",
       "        [-0.0136,  0.1262, -0.1198,  0.3189],\n",
       "        [ 0.0542,  0.0886, -0.0897,  0.2179],\n",
       "        [ 0.0299,  0.1159, -0.1550,  0.2756],\n",
       "        [ 0.1318,  0.1572, -0.1300,  0.2211],\n",
       "        [ 0.0396,  0.1629, -0.3719,  0.4132],\n",
       "        [ 0.0409,  0.1254, -0.0862,  0.3054],\n",
       "        [ 0.0547,  0.0810, -0.0989,  0.2667],\n",
       "        [-0.0168,  0.0659, -0.1262,  0.3005],\n",
       "        [ 0.1108,  0.1078, -0.0828,  0.3560],\n",
       "        [-0.0576,  0.4475,  0.1078,  0.7099],\n",
       "        [ 0.0414,  0.0283, -0.1643,  0.2602],\n",
       "        [ 0.1528,  0.1796,  0.0953,  0.5096]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CNN(V, dw, dh, out_features, weight).to('cuda')\n",
    "for X, y in dataloader_train:\n",
    "  y_pred = model(X)\n",
    "  torch.softmax(y_pred, dim=1)\n",
    "  display(y_pred)\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.394992, valid loss: 1.333407, train acc: 0.172033, valid acc: 0.369012\n",
      "train loss: 1.301437, valid loss: 1.268069, train acc: 0.520872, valid acc: 0.567365\n",
      "train loss: 1.251935, valid loss: 1.229192, train acc: 0.601367, valid acc: 0.617515\n",
      "train loss: 1.220411, valid loss: 1.202487, train acc: 0.635436, valid acc: 0.654940\n",
      "train loss: 1.197438, valid loss: 1.181763, train acc: 0.654156, valid acc: 0.678144\n",
      "train loss: 1.179151, valid loss: 1.164771, train acc: 0.663796, valid acc: 0.687874\n",
      "train loss: 1.163755, valid loss: 1.150064, train acc: 0.669319, valid acc: 0.689371\n",
      "train loss: 1.150191, valid loss: 1.136745, train acc: 0.672969, valid acc: 0.690868\n",
      "train loss: 1.137763, valid loss: 1.124402, train acc: 0.675683, valid acc: 0.694611\n",
      "train loss: 1.126097, valid loss: 1.112588, train acc: 0.676619, valid acc: 0.698353\n",
      "train loss: 1.114948, valid loss: 1.101330, train acc: 0.681486, valid acc: 0.699102\n",
      "train loss: 1.104127, valid loss: 1.090289, train acc: 0.682329, valid acc: 0.702844\n",
      "train loss: 1.093465, valid loss: 1.079433, train acc: 0.685324, valid acc: 0.706587\n",
      "train loss: 1.082905, valid loss: 1.068653, train acc: 0.687289, valid acc: 0.711078\n",
      "train loss: 1.072286, valid loss: 1.057768, train acc: 0.689817, valid acc: 0.713323\n",
      "train loss: 1.061573, valid loss: 1.046729, train acc: 0.692718, valid acc: 0.715569\n",
      "train loss: 1.050619, valid loss: 1.035383, train acc: 0.695432, valid acc: 0.715569\n",
      "train loss: 1.039410, valid loss: 1.023877, train acc: 0.698334, valid acc: 0.716317\n",
      "train loss: 1.027844, valid loss: 1.011944, train acc: 0.701891, valid acc: 0.717814\n",
      "train loss: 1.016121, valid loss: 1.000009, train acc: 0.705167, valid acc: 0.724551\n",
      "train loss: 1.004297, valid loss: 0.987939, train acc: 0.710034, valid acc: 0.729042\n",
      "train loss: 0.992298, valid loss: 0.975736, train acc: 0.713497, valid acc: 0.729790\n",
      "train loss: 0.980096, valid loss: 0.963296, train acc: 0.717147, valid acc: 0.732036\n",
      "train loss: 0.967683, valid loss: 0.950860, train acc: 0.718551, valid acc: 0.734281\n",
      "train loss: 0.954887, valid loss: 0.937840, train acc: 0.722295, valid acc: 0.740269\n",
      "train loss: 0.941571, valid loss: 0.924484, train acc: 0.725571, valid acc: 0.742515\n",
      "train loss: 0.928408, valid loss: 0.911402, train acc: 0.729596, valid acc: 0.746257\n",
      "train loss: 0.915753, valid loss: 0.898598, train acc: 0.733620, valid acc: 0.748503\n",
      "train loss: 0.903352, valid loss: 0.886165, train acc: 0.736428, valid acc: 0.752246\n",
      "train loss: 0.891109, valid loss: 0.873911, train acc: 0.739049, valid acc: 0.752246\n",
      "train loss: 0.879027, valid loss: 0.861761, train acc: 0.742699, valid acc: 0.754491\n",
      "train loss: 0.867123, valid loss: 0.849869, train acc: 0.745507, valid acc: 0.757485\n",
      "train loss: 0.855385, valid loss: 0.838167, train acc: 0.749064, valid acc: 0.759731\n",
      "train loss: 0.843836, valid loss: 0.826635, train acc: 0.752340, valid acc: 0.759731\n",
      "train loss: 0.832469, valid loss: 0.815378, train acc: 0.754680, valid acc: 0.762725\n",
      "train loss: 0.821360, valid loss: 0.804648, train acc: 0.756365, valid acc: 0.766467\n",
      "train loss: 0.810684, valid loss: 0.794259, train acc: 0.758330, valid acc: 0.766467\n",
      "train loss: 0.800387, valid loss: 0.784205, train acc: 0.759734, valid acc: 0.769461\n",
      "train loss: 0.790500, valid loss: 0.774528, train acc: 0.761419, valid acc: 0.770210\n",
      "train loss: 0.781033, valid loss: 0.765223, train acc: 0.762542, valid acc: 0.770210\n",
      "train loss: 0.771954, valid loss: 0.756278, train acc: 0.763385, valid acc: 0.770958\n",
      "train loss: 0.763261, valid loss: 0.747751, train acc: 0.764976, valid acc: 0.773204\n",
      "train loss: 0.754886, valid loss: 0.739559, train acc: 0.765537, valid acc: 0.773204\n",
      "train loss: 0.746860, valid loss: 0.731715, train acc: 0.766754, valid acc: 0.773952\n",
      "train loss: 0.739134, valid loss: 0.724137, train acc: 0.767877, valid acc: 0.774701\n",
      "train loss: 0.731714, valid loss: 0.716911, train acc: 0.768626, valid acc: 0.775449\n",
      "train loss: 0.724575, valid loss: 0.709926, train acc: 0.769094, valid acc: 0.774701\n",
      "train loss: 0.717710, valid loss: 0.703241, train acc: 0.769468, valid acc: 0.775449\n",
      "train loss: 0.711113, valid loss: 0.696821, train acc: 0.769936, valid acc: 0.775449\n",
      "train loss: 0.704752, valid loss: 0.690679, train acc: 0.770404, valid acc: 0.776946\n",
      "train loss: 0.698644, valid loss: 0.684716, train acc: 0.771153, valid acc: 0.778443\n",
      "train loss: 0.692743, valid loss: 0.678983, train acc: 0.771996, valid acc: 0.779192\n",
      "train loss: 0.687065, valid loss: 0.673448, train acc: 0.773212, valid acc: 0.779940\n",
      "train loss: 0.681561, valid loss: 0.668142, train acc: 0.773680, valid acc: 0.779192\n",
      "train loss: 0.676224, valid loss: 0.663027, train acc: 0.774429, valid acc: 0.779192\n",
      "train loss: 0.671024, valid loss: 0.658006, train acc: 0.774523, valid acc: 0.779192\n",
      "train loss: 0.666039, valid loss: 0.653130, train acc: 0.775459, valid acc: 0.780689\n",
      "train loss: 0.661230, valid loss: 0.648390, train acc: 0.776114, valid acc: 0.781437\n",
      "train loss: 0.656606, valid loss: 0.643821, train acc: 0.777143, valid acc: 0.782186\n",
      "train loss: 0.652146, valid loss: 0.639414, train acc: 0.777799, valid acc: 0.782186\n",
      "train loss: 0.647832, valid loss: 0.635134, train acc: 0.778173, valid acc: 0.782186\n",
      "train loss: 0.643649, valid loss: 0.630991, train acc: 0.778267, valid acc: 0.782934\n",
      "train loss: 0.639590, valid loss: 0.626970, train acc: 0.778641, valid acc: 0.782934\n",
      "train loss: 0.635663, valid loss: 0.623070, train acc: 0.779483, valid acc: 0.784431\n",
      "train loss: 0.631843, valid loss: 0.619320, train acc: 0.779671, valid acc: 0.784431\n",
      "train loss: 0.628153, valid loss: 0.615653, train acc: 0.779764, valid acc: 0.783683\n",
      "train loss: 0.624561, valid loss: 0.612120, train acc: 0.779951, valid acc: 0.783683\n",
      "train loss: 0.621064, valid loss: 0.608668, train acc: 0.780326, valid acc: 0.784431\n",
      "train loss: 0.617657, valid loss: 0.605315, train acc: 0.780700, valid acc: 0.784431\n",
      "train loss: 0.614335, valid loss: 0.602051, train acc: 0.780887, valid acc: 0.785180\n",
      "train loss: 0.611116, valid loss: 0.598864, train acc: 0.781355, valid acc: 0.785928\n",
      "train loss: 0.607961, valid loss: 0.595747, train acc: 0.782385, valid acc: 0.786677\n",
      "train loss: 0.604898, valid loss: 0.592741, train acc: 0.782385, valid acc: 0.786677\n",
      "train loss: 0.601910, valid loss: 0.589808, train acc: 0.782853, valid acc: 0.786677\n",
      "train loss: 0.598982, valid loss: 0.586940, train acc: 0.783134, valid acc: 0.786677\n",
      "train loss: 0.596121, valid loss: 0.584128, train acc: 0.783321, valid acc: 0.787425\n",
      "train loss: 0.593321, valid loss: 0.581374, train acc: 0.783508, valid acc: 0.788174\n",
      "train loss: 0.590581, valid loss: 0.578639, train acc: 0.784163, valid acc: 0.788922\n",
      "train loss: 0.587891, valid loss: 0.576025, train acc: 0.784631, valid acc: 0.788922\n",
      "train loss: 0.585259, valid loss: 0.573442, train acc: 0.784912, valid acc: 0.789671\n",
      "train loss: 0.582674, valid loss: 0.570909, train acc: 0.785286, valid acc: 0.788922\n",
      "train loss: 0.580136, valid loss: 0.568422, train acc: 0.785286, valid acc: 0.788922\n",
      "train loss: 0.577640, valid loss: 0.565976, train acc: 0.786035, valid acc: 0.790419\n",
      "train loss: 0.575197, valid loss: 0.563610, train acc: 0.786597, valid acc: 0.788922\n",
      "train loss: 0.572795, valid loss: 0.561262, train acc: 0.786784, valid acc: 0.788922\n",
      "train loss: 0.570433, valid loss: 0.558911, train acc: 0.787158, valid acc: 0.789671\n",
      "train loss: 0.568108, valid loss: 0.556647, train acc: 0.787626, valid acc: 0.789671\n",
      "train loss: 0.565813, valid loss: 0.554344, train acc: 0.787720, valid acc: 0.790419\n",
      "train loss: 0.563567, valid loss: 0.552141, train acc: 0.788094, valid acc: 0.791168\n",
      "train loss: 0.561366, valid loss: 0.549959, train acc: 0.788562, valid acc: 0.791168\n",
      "train loss: 0.559193, valid loss: 0.547841, train acc: 0.788562, valid acc: 0.790419\n",
      "train loss: 0.557064, valid loss: 0.545709, train acc: 0.789030, valid acc: 0.790419\n",
      "train loss: 0.554951, valid loss: 0.543600, train acc: 0.788750, valid acc: 0.791168\n",
      "train loss: 0.552887, valid loss: 0.541537, train acc: 0.789686, valid acc: 0.791916\n",
      "train loss: 0.550847, valid loss: 0.539532, train acc: 0.790154, valid acc: 0.791916\n",
      "train loss: 0.548833, valid loss: 0.537537, train acc: 0.790247, valid acc: 0.791168\n",
      "train loss: 0.546842, valid loss: 0.535507, train acc: 0.790528, valid acc: 0.791916\n",
      "train loss: 0.544873, valid loss: 0.533558, train acc: 0.790996, valid acc: 0.792665\n",
      "train loss: 0.542937, valid loss: 0.531650, train acc: 0.791651, valid acc: 0.792665\n",
      "train loss: 0.541021, valid loss: 0.529769, train acc: 0.792025, valid acc: 0.792665\n"
     ]
    }
   ],
   "source": [
    "model = CNN(V, dw, dh, out_features, weight).to('cuda')\n",
    "optimizer = optim.SGD(params=model.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "\n",
    "for epoch in range(100):\n",
    "  model.train()\n",
    "  loss_train = 0\n",
    "  correct = 0\n",
    "  for X, y in dataloader_train:\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X)\n",
    "\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.softmax(y_pred, dim=1)\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    correct += torch.sum(y_pred == y).item()\n",
    "\n",
    "    loss_train += loss.item()*X.size(0)\n",
    "\n",
    "  loss_train /= len(dataloader_train.dataset)\n",
    "  acc_train = correct / len(dataloader_train.dataset)\n",
    "\n",
    "  model.eval()\n",
    "  loss_valid = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader_valid:\n",
    "      y_pred = model(X)\n",
    "      loss = criterion(y_pred, y)\n",
    "\n",
    "      y_pred = torch.softmax(y_pred, dim=1)\n",
    "      y_pred = y_pred.argmax(dim=1)\n",
    "      correct += torch.sum(y_pred == y).item()\n",
    "\n",
    "      loss_valid += loss.item()*X.size(0)\n",
    "\n",
    "  loss_valid /= len(dataloader_valid.dataset)\n",
    "  acc_valid = correct / len(dataloader_valid.dataset)\n",
    "\n",
    "  print(f\"train loss: {loss_train:>7f}, valid loss: {loss_valid:>7f}, train acc: {acc_train:>7f}, valid acc: {acc_valid:>7f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "  def __init__(self, patience=10, verbose=False, path='model/checkpoint.pth'):\n",
    "    self.patience = patience\n",
    "    self.verbose = verbose\n",
    "    self.counter = 0\n",
    "    self.early_stop = False\n",
    "    self.best = torch.inf\n",
    "    self.path = path\n",
    "\n",
    "  def __call__(self, val_loss, model):\n",
    "    if val_loss < self.best:\n",
    "      self.checkpoint(val_loss, model)\n",
    "      self.counter = 0\n",
    "    else:\n",
    "      self.counter += 1\n",
    "      if self.verbose:\n",
    "        print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "      if self.counter >= self.patience:\n",
    "        self.early_stop = True\n",
    "\n",
    "  def checkpoint(self, val_loss, model):\n",
    "    if self.verbose:\n",
    "        print(f'Validation loss decreased ({self.best:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "    torch.save(model.state_dict(), self.path)\n",
    "    self.best = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.271921, valid loss: 0.029629, train acc: 0.931018, valid acc: 0.997006\n",
      "train loss: 0.020503, valid loss: 0.010149, train acc: 0.996162, valid acc: 0.998503\n",
      "train loss: 0.009222, valid loss: 0.007931, train acc: 0.997847, valid acc: 0.998503\n",
      "train loss: 0.006706, valid loss: 0.007182, train acc: 0.998222, valid acc: 0.997006\n",
      "train loss: 0.006014, valid loss: 0.008315, train acc: 0.998222, valid acc: 0.997006\n",
      "train loss: 0.005506, valid loss: 0.006582, train acc: 0.998222, valid acc: 0.997006\n",
      "train loss: 0.006027, valid loss: 0.004798, train acc: 0.997941, valid acc: 0.997754\n",
      "train loss: 0.005650, valid loss: 0.007021, train acc: 0.997847, valid acc: 0.997006\n",
      "train loss: 0.004571, valid loss: 0.006354, train acc: 0.998409, valid acc: 0.997006\n",
      "train loss: 0.005551, valid loss: 0.005732, train acc: 0.998222, valid acc: 0.997754\n",
      "train loss: 0.005156, valid loss: 0.003844, train acc: 0.998502, valid acc: 0.998503\n",
      "train loss: 0.004689, valid loss: 0.007049, train acc: 0.998409, valid acc: 0.997754\n",
      "train loss: 0.004910, valid loss: 0.006706, train acc: 0.998315, valid acc: 0.997754\n",
      "train loss: 0.004963, valid loss: 0.002877, train acc: 0.998315, valid acc: 0.998503\n",
      "train loss: 0.005908, valid loss: 0.006193, train acc: 0.998128, valid acc: 0.997754\n",
      "train loss: 0.005160, valid loss: 0.006497, train acc: 0.998128, valid acc: 0.997006\n",
      "train loss: 0.004040, valid loss: 0.005730, train acc: 0.998502, valid acc: 0.998503\n",
      "train loss: 0.005252, valid loss: 0.008142, train acc: 0.998222, valid acc: 0.997006\n",
      "train loss: 0.004201, valid loss: 0.001808, train acc: 0.998502, valid acc: 0.999251\n",
      "train loss: 0.004713, valid loss: 0.006976, train acc: 0.998222, valid acc: 0.997006\n",
      "train loss: 0.004428, valid loss: 0.001545, train acc: 0.998222, valid acc: 1.000000\n",
      "train loss: 0.003981, valid loss: 0.009384, train acc: 0.998596, valid acc: 0.997006\n",
      "train loss: 0.004371, valid loss: 0.003072, train acc: 0.998596, valid acc: 0.998503\n",
      "train loss: 0.004366, valid loss: 0.007580, train acc: 0.998409, valid acc: 0.997006\n",
      "train loss: 0.004590, valid loss: 0.003717, train acc: 0.998128, valid acc: 0.997754\n",
      "train loss: 0.003996, valid loss: 0.007897, train acc: 0.998502, valid acc: 0.996257\n",
      "train loss: 0.004344, valid loss: 0.003493, train acc: 0.998315, valid acc: 0.998503\n",
      "train loss: 0.004346, valid loss: 0.005659, train acc: 0.998409, valid acc: 0.997006\n",
      "train loss: 0.003893, valid loss: 0.003014, train acc: 0.998409, valid acc: 0.999251\n",
      "train loss: 0.003922, valid loss: 0.008406, train acc: 0.998502, valid acc: 0.996257\n",
      "train loss: 0.004403, valid loss: 0.001718, train acc: 0.998502, valid acc: 0.999251\n",
      "-- Early Stopping --\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "\n",
    "model = CNN(V, dw, dh, out_features, weight).to('cuda')\n",
    "optimizer = optim.AdamW(params=model.parameters())\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, 1e-5)\n",
    "criterion = nn.CrossEntropyLoss().to('cuda')\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  loss_train = 0\n",
    "  correct = 0\n",
    "  for X, y in dataloader_train:\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X)\n",
    "\n",
    "    loss = criterion(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.softmax(y_pred, dim=1)\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    correct += torch.sum(y_pred == y).item()\n",
    "\n",
    "    loss_train += loss.item()*X.size(0)\n",
    "\n",
    "  loss_train /= len(dataloader_train.dataset)\n",
    "  acc_train = correct / len(dataloader_train.dataset)\n",
    "\n",
    "  model.eval()\n",
    "  loss_valid = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader_valid:\n",
    "      y_pred = model(X)\n",
    "      loss = criterion(y_pred, y)\n",
    "\n",
    "      y_pred = torch.softmax(y_pred, dim=1)\n",
    "      y_pred = y_pred.argmax(dim=1)\n",
    "      correct += torch.sum(y_pred == y).item()\n",
    "\n",
    "      loss_valid += loss.item()*X.size(0)\n",
    "\n",
    "  loss_valid /= len(dataloader_valid.dataset)\n",
    "  acc_valid = correct / len(dataloader_valid.dataset)\n",
    "\n",
    "  print(f\"train loss: {loss_train:>7f}, valid loss: {loss_valid:>7f}, train acc: {acc_train:>7f}, valid acc: {acc_valid:>7f}\")\n",
    "\n",
    "  early_stopping(loss_valid, model)\n",
    "  if early_stopping.early_stop:\n",
    "    print('-- Early Stopping --')\n",
    "    break\n",
    "\n",
    "  scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "  def __init__(self, X, y, tokenizer, max_len) -> None:\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = self.X[idx]\n",
    "    token = self.tokenizer.encode_plus(text, max_length=self.max_len, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = token['input_ids']\n",
    "    token_type_ids = token['token_type_ids']\n",
    "    attention_mask = token['attention_mask']\n",
    "\n",
    "    return {\n",
    "      'input_ids': torch.LongTensor(input_ids).squeeze(0).to('cuda'),\n",
    "      'token_type_ids': torch.LongTensor(token_type_ids).squeeze(0).to('cuda'),\n",
    "      'attention_mask': torch.LongTensor(attention_mask).squeeze(0).to('cuda'),\n",
    "      'labels': torch.Tensor(self.y[idx])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = title[:]\n",
    "X_train, X_valtest = X[:len(train)], X[len(train):]\n",
    "X_valid, X_test = X_valtest[:len(valid)], X_valtest[len(valid):]\n",
    "\n",
    "y = [category_map[i] for i in labels]\n",
    "y = torch.tensor(y)\n",
    "y_train, y_valtest = y[:len(train)], y[len(train):]\n",
    "y_valid, y_test = y[:len(valid)], y[len(valid):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = 20\n",
    "\n",
    "dataset_train = NewsDataset(X_train, y_train.to('cuda'), tokenizer, max_len)\n",
    "dataset_valid = NewsDataset(X_valid, y_valid.to('cuda'), tokenizer, max_len)\n",
    "dataset_test = NewsDataset(X_test, y_test.to('cuda'), tokenizer, max_len)\n",
    "dataloader_train = DataLoader(dataset_train, 256, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, 64)\n",
    "dataloader_test = DataLoader(dataset_test, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 100, 100, 100, 100, 100, 100, 100, 100, 100, 3263, 102, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(X_train[0], max_length=max_len, padding='max_length', truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "  def __init__(self, p, out_features):\n",
    "    super().__init__()\n",
    "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "    self.drop = nn.Dropout(p)\n",
    "    self.fc = nn.Linear(768, out_features)\n",
    "\n",
    "  def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "    expanded_mask = attention_mask.unsqueeze(-1)\n",
    "    masked_hidden_state = last_hidden_state*expanded_mask\n",
    "\n",
    "    max_pooled = torch.max(masked_hidden_state + (1-expanded_mask)*-1e9, dim=1)[0]\n",
    "    max_pooled = self.drop(max_pooled)\n",
    "\n",
    "    x = self.fc(self.drop(max_pooled))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 1.068996, valid loss: 0.849781, train acc: 0.575534, valid acc: 0.420659\n",
      "train loss: 0.811935, valid loss: 0.713185, train acc: 0.709940, valid acc: 0.413922\n",
      "train loss: 0.721582, valid loss: 0.703723, train acc: 0.740079, valid acc: 0.403443\n",
      "train loss: 0.667735, valid loss: 0.667069, train acc: 0.760670, valid acc: 0.401198\n",
      "train loss: 0.634880, valid loss: 0.508331, train acc: 0.770592, valid acc: 0.395958\n",
      "train loss: 0.608184, valid loss: 0.648385, train acc: 0.778922, valid acc: 0.396707\n",
      "train loss: 0.585423, valid loss: 0.553524, train acc: 0.787346, valid acc: 0.407186\n",
      "train loss: 0.567084, valid loss: 0.674349, train acc: 0.795021, valid acc: 0.403443\n",
      "train loss: 0.548574, valid loss: 0.598040, train acc: 0.802040, valid acc: 0.392216\n",
      "train loss: 0.525514, valid loss: 0.476576, train acc: 0.808218, valid acc: 0.390719\n",
      "train loss: 0.507952, valid loss: 0.561907, train acc: 0.816735, valid acc: 0.399701\n",
      "train loss: 0.496509, valid loss: 0.482738, train acc: 0.820105, valid acc: 0.391467\n",
      "train loss: 0.486582, valid loss: 0.513062, train acc: 0.822070, valid acc: 0.393713\n",
      "train loss: 0.475366, valid loss: 0.429988, train acc: 0.827031, valid acc: 0.392964\n",
      "train loss: 0.468720, valid loss: 0.550408, train acc: 0.828341, valid acc: 0.401198\n",
      "train loss: 0.459812, valid loss: 0.415430, train acc: 0.833677, valid acc: 0.395958\n",
      "train loss: 0.451974, valid loss: 0.434817, train acc: 0.834051, valid acc: 0.391467\n",
      "train loss: 0.450098, valid loss: 0.332050, train acc: 0.836297, valid acc: 0.392964\n",
      "train loss: 0.449215, valid loss: 0.571405, train acc: 0.833677, valid acc: 0.390719\n",
      "train loss: 0.438726, valid loss: 0.452168, train acc: 0.836016, valid acc: 0.392216\n",
      "train loss: 0.436005, valid loss: 0.459589, train acc: 0.838263, valid acc: 0.385479\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m   y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(y_pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m   y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m   correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m   loss_train \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     30\u001b[0m loss_train \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataloader_train\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "\n",
    "model = BERT(0.3, 4).to('cuda')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4).to('cuda')\n",
    "optimizer = optim.AdamW(model.parameters(), 1e-5)\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, num_epochs, 1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping()\n",
    "s = set()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  model.train()\n",
    "  loss_train = 0\n",
    "  correct = 0\n",
    "  for data in dataloader_train:\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(input_ids=data['input_ids'], token_type_ids=data['token_type_ids'], attention_mask=data['attention_mask'])\n",
    "\n",
    "    loss = criterion(y_pred, data['labels'])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    y_pred = torch.softmax(y_pred, dim=1)\n",
    "    y_pred = y_pred.argmax(dim=1)\n",
    "    correct += torch.sum(y_pred == data['labels']).item()\n",
    "\n",
    "    loss_train += loss.item()*data['input_ids'].size(0)\n",
    "\n",
    "  loss_train /= len(dataloader_train.dataset)\n",
    "  acc_train = correct / len(dataloader_train.dataset)\n",
    "\n",
    "  model.eval()\n",
    "  loss_valid = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data in dataloader_valid:\n",
    "\n",
    "      y_pred = model(input_ids=data['input_ids'], token_type_ids=data['token_type_ids'], attention_mask=data['attention_mask'])\n",
    "\n",
    "      y_pred = torch.softmax(y_pred, dim=1)\n",
    "      y_pred = y_pred.argmax(dim=1)\n",
    "      correct += torch.sum(y_pred == data['labels']).item()\n",
    "\n",
    "      loss_valid += loss.item()*data['input_ids'].size(0)\n",
    "\n",
    "  loss_valid /= len(dataloader_valid.dataset)\n",
    "  acc_valid = correct / len(dataloader_valid.dataset)\n",
    "\n",
    "  print(f\"train loss: {loss_train:>7f}, valid loss: {loss_valid:>7f}, train acc: {acc_train:>7f}, valid acc: {acc_valid:>7f}\")\n",
    "\n",
    "  early_stopping(loss_valid, model)\n",
    "  if early_stopping.early_stop:\n",
    "    print('-- Early Stopping --')\n",
    "    break\n",
    "\n",
    "  # scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
