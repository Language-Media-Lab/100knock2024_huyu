{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# 解凍されたai.ja.txtファイルのパス\n",
    "input_file = 'ai.ja.txt'  # 解凍したテキストファイルのパス\n",
    "output_file = 'ai.ja.txt.parsed'  # 結果を保存するファイル名\n",
    "\n",
    "# CaboChaを使った解析\n",
    "# # CaboChaをコマンドラインで実行\n",
    "cabocha_command = ['cabocha', '-f1', input_file]\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    subprocess.run(cabocha_command, stdout=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'surface': '『', 'base': '『', 'pos': '記号', 'pos1': '括弧開'}\n",
      "{'surface': '日本', 'base': '日本', 'pos': '名詞', 'pos1': '固有名詞'}\n",
      "{'surface': '大', 'base': '大', 'pos': '接頭詞', 'pos1': '名詞接続'}\n",
      "{'surface': '百科全書', 'base': '百科全書', 'pos': '名詞', 'pos1': '一般'}\n",
      "{'surface': '(', 'base': '*', 'pos': '名詞', 'pos1': 'サ変接続'}\n",
      "{'surface': 'ニッポニカ', 'base': '*', 'pos': '名詞', 'pos1': '一般'}\n",
      "{'surface': ')』', 'base': '*', 'pos': '名詞', 'pos1': 'サ変接続'}\n",
      "{'surface': 'の', 'base': 'の', 'pos': '助詞', 'pos1': '連体化'}\n",
      "{'surface': '解説', 'base': '解説', 'pos': '名詞', 'pos1': 'サ変接続'}\n",
      "{'surface': 'で', 'base': 'で', 'pos': '助詞', 'pos1': '格助詞'}\n",
      "{'surface': '、', 'base': '、', 'pos': '記号', 'pos1': '読点'}\n",
      "{'surface': '情報', 'base': '情報', 'pos': '名詞', 'pos1': '一般'}\n",
      "{'surface': '工学', 'base': '工学', 'pos': '名詞', 'pos1': '一般'}\n",
      "{'surface': '者', 'base': '者', 'pos': '名詞', 'pos1': '接尾'}\n",
      "{'surface': '・', 'base': '・', 'pos': '記号', 'pos1': '一般'}\n",
      "{'surface': '通信', 'base': '通信', 'pos': '名詞', 'pos1': 'サ変接続'}\n",
      "{'surface': '工学', 'base': '工学', 'pos': '名詞', 'pos1': '一般'}\n",
      "{'surface': '者', 'base': '者', 'pos': '名詞', 'pos1': '接尾'}\n",
      "{'surface': 'の', 'base': 'の', 'pos': '助詞', 'pos1': '連体化'}\n",
      "{'surface': '佐藤', 'base': '佐藤', 'pos': '名詞', 'pos1': '固有名詞'}\n",
      "{'surface': '理', 'base': '理', 'pos': '名詞', 'pos1': '一般'}\n",
      "{'surface': '史', 'base': '史', 'pos': '名詞', 'pos1': '接尾'}\n",
      "{'surface': 'は', 'base': 'は', 'pos': '助詞', 'pos1': '係助詞'}\n",
      "{'surface': '次', 'base': '次', 'pos': '名詞', 'pos1': '一般'}\n",
      "{'surface': 'の', 'base': 'の', 'pos': '助詞', 'pos1': '連体化'}\n",
      "{'surface': 'よう', 'base': 'よう', 'pos': '名詞', 'pos1': '非自立'}\n",
      "{'surface': 'に', 'base': 'に', 'pos': '助詞', 'pos1': '副詞化'}\n",
      "{'surface': '述べ', 'base': '述べる', 'pos': '動詞', 'pos1': '自立'}\n",
      "{'surface': 'て', 'base': 'て', 'pos': '助詞', 'pos1': '接続助詞'}\n",
      "{'surface': 'いる', 'base': 'いる', 'pos': '動詞', 'pos1': '非自立'}\n",
      "{'surface': '。', 'base': '。', 'pos': '記号', 'pos1': '句点'}\n"
     ]
    }
   ],
   "source": [
    "#40\n",
    "\n",
    "class Morph:#形態素\n",
    "    def __init__(self,line):\n",
    "        part = line.split(\"\\t\")\n",
    "        part_=part[1].split(\",\")\n",
    "        self.surface=part[0]\n",
    "        self.base=part_[-3]\n",
    "        self.pos=part_[0]\n",
    "        self.pos1=part_[1]\n",
    "    \n",
    "def read_hito_(file_path):\n",
    "    sentences=[]\n",
    "    sentence=[]\n",
    "    with open(file_path,\"r\",encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line[0]==\"*\":\n",
    "                continue\n",
    "            elif line !=\"EOS\\n\":\n",
    "                sentence.append(Morph(line))\n",
    "            if line ==\"EOS\\n\":\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence=[]\n",
    "    return sentences  \n",
    "\n",
    "pased_file=\"ai.ja.txt.parsed\"\n",
    "sentences=read_hito_(pased_file)\n",
    "for i in sentences[2]:\n",
    "    print(vars(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文節 0: '日本大百科全書(ニッポニカ)』の' -> 係り先: '解説で'\n",
      "文節 1: '解説で' -> 係り先: '述べている'\n",
      "文節 2: '情報工学者通信工学者の' -> 係り先: '佐藤理史は'\n",
      "文節 3: '佐藤理史は' -> 係り先: '述べている'\n",
      "文節 4: '次のように' -> 係り先: '述べている'\n",
      "文節 5: '述べている' -> 係り先: 'なし'\n"
     ]
    }
   ],
   "source": [
    "#41\n",
    "   \n",
    "class Chunk:#文節\n",
    "    def __init__(self,morphs,dst):\n",
    "        self.morphs=morphs #（Morphオブジェクト）のリスト\n",
    "        self.dst=dst #係り先文節インデックス番号\n",
    "        self.srcs=[] #係り元文節インデックス番号のリスト\n",
    "        \n",
    "    def get_surface(self):\n",
    "        surface=[]\n",
    "        for morph in self.morphs:\n",
    "            if morph.pos!=\"記号\":\n",
    "                surface.append(morph.surface)\n",
    "        result=\"\".join(surface)\n",
    "        return result\n",
    "    \n",
    "class Sentence:#文\n",
    "    def __init__(self,chunks):\n",
    "        self.chunks=chunks\n",
    "        for i, chunk in enumerate(self.chunks):\n",
    "            if chunk.dst!=-1:  # 有効な係り先文節の場合\n",
    "                self.chunks[chunk.dst].srcs.append(i)  # 係り先文節のsrcsに係り元の文節インデックスiを追加\n",
    "\n",
    "        \n",
    "def read_hito(file_path):\n",
    "    sentences=[]#文\n",
    "    chunks=[]#文節\n",
    "    morphs=[]#形態素\n",
    "    dst=-1#インデックス番号\n",
    "    \n",
    "    with open(file_path,\"r\",encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line[0]==\"*\":\n",
    "                if morphs:\n",
    "                    chunks.append(Chunk(morphs,dst))\n",
    "                    morphs=[]\n",
    "                dst = int(line.split(' ')[2].replace(\"D\", \"\"))\n",
    "            elif line !=\"EOS\\n\":\n",
    "                morphs.append(Morph(line))\n",
    "            if line ==\"EOS\\n\":\n",
    "                if morphs:\n",
    "                    chunks.append(Chunk(morphs,dst))\n",
    "                if chunks:\n",
    "                    sentences.append(Sentence(chunks))\n",
    "                chunks=[]\n",
    "                morphs=[]\n",
    "                dst=-1\n",
    "    return sentences  \n",
    "\n",
    "pased_file=\"ai.ja.txt.parsed\"\n",
    "sentences=read_hito(pased_file)\n",
    "for i, chunk in enumerate(sentences[2].chunks):\n",
    "    surface = chunk.get_surface()\n",
    "    dst = chunk.dst\n",
    "    if dst != -1 and dst < len(sentences[2].chunks):  # 範囲外を防いでいるか\n",
    "        dst_surface = sentences[2].chunks[dst].get_surface()\n",
    "    else:\n",
    "        dst_surface = \"なし\"\n",
    "    print(f\"文節 {i}: '{surface}' -> 係り先: '{dst_surface}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本大百科全書(ニッポニカ)』の\t解説で\n",
      "解説で\t述べている\n",
      "情報工学者通信工学者の\t佐藤理史は\n",
      "佐藤理史は\t述べている\n",
      "次のように\t述べている\n"
     ]
    }
   ],
   "source": [
    "#42\n",
    "for chunk in sentences[2].chunks:\n",
    "    src_surface = chunk.get_surface()\n",
    "    if chunk.dst != -1:  # 係り先がある場合\n",
    "        dst_surface = sentences[2].chunks[chunk.dst].get_surface()\n",
    "        print(f\"{src_surface}\\t{dst_surface}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解説で\t述べている\n",
      "解説で\t述べている\n",
      "佐藤理史は\t述べている\n",
      "佐藤理史は\t述べている\n",
      "佐藤理史は\t述べている\n",
      "佐藤理史は\t述べている\n",
      "佐藤理史は\t述べている\n",
      "佐藤理史は\t述べている\n",
      "次のように\t述べている\n",
      "次のように\t述べている\n",
      "次のように\t述べている\n",
      "次のように\t述べている\n"
     ]
    }
   ],
   "source": [
    "#43\n",
    "\n",
    "for chunk in sentences[2].chunks:\n",
    "    src_surface = chunk.get_surface()\n",
    "    if chunk.dst != -1:  # 係り先がある場合\n",
    "        dst_surface = sentences[2].chunks[chunk.dst].get_surface()\n",
    "        for morph in chunk.morphs:\n",
    "            if morph.pos==\"名詞\":\n",
    "                for morph in sentences[2].chunks[chunk.dst].morphs:\n",
    "                    if morph.pos==\"動詞\":\n",
    "                        print(f\"{src_surface}\\t{dst_surface}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: graphviz in c:\\users\\横山　明咲\\appdata\\roaming\\python\\python311\\site-packages (0.20.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "グラフが生成されました。\n"
     ]
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# 簡単なグラフを作成\n",
    "dot = Digraph()\n",
    "dot.node('A', 'Hello')\n",
    "dot.node('B', 'World')\n",
    "dot.edge('A', 'B')\n",
    "\n",
    "# グラフをファイルとして保存\n",
    "dot.render('test_graph', format='png', view=False)\n",
    "print(\"グラフが生成されました。\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "係り受け木を'sentence2_dependency_tree.png'として保存しました。\n"
     ]
    }
   ],
   "source": [
    "#44\n",
    "from graphviz import Digraph\n",
    "\n",
    "def visualize_dependency_tree(sentence, output_file=\"dependency_tree\"):\n",
    "    # 有向グラフを作成\n",
    "    dot = Digraph(format=\"png\")\n",
    "    dot.attr('node', shape='box', fontname=\"MS Gothic\")  # フォントを日本語対応に設定\n",
    "    \n",
    "    # 文節をノードとして追加し、係り受け関係をエッジで追加\n",
    "    for i, chunk in enumerate(sentence.chunks):\n",
    "        surface = chunk.get_surface()\n",
    "        dot.node(f\"{i}\", surface)  # ノードのラベルは文節の表層形\n",
    "        if chunk.dst != -1:  # 有効係り先がある場合\n",
    "            dst_surface = sentence.chunks[chunk.dst].get_surface()\n",
    "            dot.edge(f\"{i}\", f\"{chunk.dst}\")  # エッジを追加\n",
    "\n",
    "    dot.render(output_file, cleanup=True)\n",
    "    print(f\"係り受け木を'{output_file}.png'として保存しました。\")\n",
    "\n",
    "visualize_dependency_tree(sentences[2], output_file=\"sentence2_dependency_tree\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用いる\tを\n",
      "する\tて を\n",
      "指す\tを\n",
      "代わる\tに を\n",
      "行う\tて に\n",
      "する\tと も\n"
     ]
    }
   ],
   "source": [
    "#45\n",
    "from collections import defaultdict\n",
    "\n",
    "# 出力をファイルに保存しつつ、printで表示\n",
    "with open('./ans45.txt', 'w', encoding='utf-8') as f:\n",
    "    for sentence in sentences:\n",
    "        # 文の中の各chunkを処理\n",
    "        for chunk in sentence.chunks:\n",
    "            # 文節内の最左の動詞を取得\n",
    "            verb = None\n",
    "            for morph in chunk.morphs:\n",
    "                if morph.pos == '動詞':  # 動詞を探す\n",
    "                    verb = morph.base  # 動詞の基本形を取得\n",
    "                    break  # 最左の動詞を見つけたら処理を終了\n",
    "            \n",
    "            if verb==None:  # 動詞がない場合はスキップ\n",
    "                continue\n",
    "\n",
    "            cases = []  # 助詞リスト\n",
    "            for src in chunk.srcs:\n",
    "                # 範囲内の文節を確認して処理\n",
    "                if 0 <= src < len(sentence.chunks):\n",
    "                    src_chunk = sentence.chunks[src]  # 係り元の文節\n",
    "                    # 助詞を抽出\n",
    "                    particles = [m.surface for m in src_chunk.morphs if m.pos == '助詞']\n",
    "                    if particles:  # 助詞が見つかった場合\n",
    "                        cases.extend(particles)  # 助詞をリストに追加\n",
    "\n",
    "            # 助詞が見つかった場合のみ出力\n",
    "            if cases:\n",
    "                # 重複を除去し辞書順にソート\n",
    "                cases = sorted(set(cases))\n",
    "                # 出力形式: 述語（動詞） + 助詞\n",
    "                line = f\"{verb}\\t{' '.join(cases)}\"\n",
    "\n",
    "                # ファイルに書き込む\n",
    "                print(line, file=f)\n",
    "\n",
    "                if sentence==sentences[2]:\n",
    "                    print(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cat' �́A�����R�}���h�܂��͊O���R�}���h�A\n",
      "����\\�ȃv���O�����܂��̓o�b�` �t�@�C���Ƃ��ĔF������Ă��܂���B\n",
      "'cat' �́A�����R�}���h�܂��͊O���R�}���h�A\n",
      "����\\�ȃv���O�����܂��̓o�b�` �t�@�C���Ƃ��ĔF������Ă��܂���B\n",
      "'cat' �́A�����R�}���h�܂��͊O���R�}���h�A\n",
      "����\\�ȃv���O�����܂��̓o�b�` �t�@�C���Ƃ��ĔF������Ă��܂���B\n"
     ]
    }
   ],
   "source": [
    "!cat \"/c/Users/横山　明咲/100knock2024_huyu/chapter05/ans45.txt\" | grep '^行う' | sort | uniq -c | sort -nr   \n",
    "!cat \"/c/Users/横山　明咲/100knock2024_huyu/chapter05/ans45.txt\" | grep '^なる' | sort | uniq -c | sort -nr\n",
    "!cat \"/c/Users/横山　明咲/100knock2024_huyu/chapter05/ans45.txt\" | grep '^与える' | sort | uniq -c | sort -nr      \n",
    "\n",
    "#文字化けが発生したため、gitbushで実行したら出力された\n",
    "#出力例（与える））\n",
    "#      1 与える  に は を\n",
    "#      1 与える  が に\n",
    "#      1 与える  が など に"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "述べる\tで に は\t解説で、 次のように 佐藤理史は\n"
     ]
    }
   ],
   "source": [
    "#46\n",
    "# 出力をファイルに保存しつつ、printで表示\n",
    "with open('./ans46.txt', 'w', encoding='utf-8') as f:\n",
    "    for sentence in sentences:\n",
    "        # 文の中の各chunkを処理\n",
    "        for chunk in sentence.chunks:\n",
    "            # 文節内の最左の動詞を取得\n",
    "            verb = None\n",
    "            for morph in chunk.morphs:\n",
    "                if morph.pos == '動詞':  # 動詞を探す\n",
    "                    verb = morph.base  # 動詞の基本形を取得\n",
    "                    break  # 最左の動詞を見つけたら処理を終了\n",
    "            \n",
    "            if verb==None:  # 動詞がない場合はスキップ\n",
    "                continue\n",
    "\n",
    "            # 動詞に係る文節の助詞と項を収集\n",
    "            cases_and_items = []  # 格（助詞）と項（文節そのもの）のペア\n",
    "            for src in chunk.srcs:\n",
    "                # 範囲内の文節を確認して処理\n",
    "                if 0 <= src < len(sentence.chunks):\n",
    "                    src_chunk = sentence.chunks[src]  # 係り元の文節\n",
    "                    # 助詞を抽出\n",
    "                    particles = [m.surface for m in src_chunk.morphs if m.pos == '助詞']\n",
    "                    if particles:  # 助詞が見つかった場合\n",
    "                        item = ''.join(m.surface for m in src_chunk.morphs)  # 文節そのもの（単語列）\n",
    "                        cases_and_items.append((particles[-1], item))  # 最右の助詞を格とする\n",
    "\n",
    "            # 格と項を辞書順に整列\n",
    "            if cases_and_items:\n",
    "                # 助詞を基準にソート\n",
    "                cases_and_items = sorted(cases_and_items, key=lambda x: x[0])\n",
    "                cases = [ci[0] for ci in cases_and_items]  # 助詞\n",
    "                items = [ci[1] for ci in cases_and_items]  # 文節\n",
    "\n",
    "                # 出力形式: 述語（動詞） + 助詞 + 項\n",
    "                line = f\"{verb}\\t{' '.join(cases)}\\t{' '.join(items)}\"\n",
    "\n",
    "                # ファイルに書き込む\n",
    "                print(line, file=f)\n",
    "\n",
    "                # Notebookに出力として表示\n",
    "                print(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行動を代わる\tに\t人間に\n",
      "記述をする\tと\t主体と\n",
      "注目を集める\tが\t「サポートベクターマシン」が\n",
      "経験を行う\tに を\t元に 学習を\n",
      "学習を行う\tに を\t元に 経験を\n",
      "学習をする\tて に は を を通して\tなされている。 元に ACT-Rでは、 推論ルールを、 生成規則を通して\n",
      "進化を見せる\tて て において は\t活躍している。 加えて、 生成技術において （敵対的生成ネットワーク）は、\n",
      "開発を行う\tは\tエイダ・ラブレスは\n",
      "処理を行う\tに に により\t同年に、 Webに ティム・バーナーズ＝リーにより、\n",
      "意味をする\tに\tデータに\n",
      "処理を行う\tて に\t付加して、 コンピュータに\n",
      "研究を進める\tて\t費やして\n",
      "命令をする\tで\t機構で\n",
      "運転をする\tに\t元に\n",
      "特許をする\tが に\t日本が 2018年までに\n",
      "運転をする\tて に\t基づいて、 柔軟に\n",
      "注目を集める\tから は\tことから、 ファジィは\n",
      "制御を用いる\tて も\t受けて、 他社も\n",
      "制御をする\tから\t少なさから、\n",
      "改善を果たす\tが で に\tチームが 画像処理コンテストで 2012年に\n",
      "研究を続ける\tが て\tジェフ・ホーキンスが、 向けて\n",
      ")をする\tは\t8月には、\n",
      "注目を集める\tに\t急速に\n",
      "投資を行う\tで に\t民間企業主導で 全世界的に\n",
      "探索を行う\tで\t無報酬で\n",
      "推論をする\tて\t経て\n",
      "研究を始める\tとも は\tマックスプランク研究所とも Googleは、\n",
      "研究を行う\tて\t始めており、\n",
      "開発をする\tで は\t官民一体で 中国では\n",
      "開発をする\tで\t日本で\n",
      "投資をする\tに は\t2022年までに 韓国は、\n",
      "反乱を起こす\tて に対して\t於いて、 人間に対して\n",
      "監視を行う\tに まで\t人工知能に 歩行者まで\n",
      "手続きを経る\tを\tウイグル族を\n",
      "制御をする\tは\tAIプログラムは\n",
      "判断を介す\tから\t観点から\n",
      "禁止を求める\tが は\tヒューマン・ライツ・ウォッチが、 4月には\n",
      "競争を行う\tは をめぐって\t米国・中国・ロシアは 軍事利用をめぐって\n",
      "追及を受ける\tて で と とともに は\t暴露されており、 整合性で 拒否すると 「」とともに、 公聴会では、\n",
      "研究をする\tが\tMicrosoftが\n",
      "解任をする\tて は\t含まれており、 Google社員らは\n",
      "解散をする\tが で は\t倫理委員会が 理由で、 Googleは\n",
      "存在を見いだす\tに\tものに\n",
      "話をする\tは\t哲学者は\n",
      "議論を行う\tまで\t「これまで\n"
     ]
    }
   ],
   "source": [
    "#47\n",
    "from collections import defaultdict\n",
    "\n",
    "with open('./ans47.txt', 'w', encoding='utf-8') as f:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence.chunks:\n",
    "            # 「サ変接続名詞+を」で構成される文節か判定\n",
    "            sahen_wo = None\n",
    "            for i, morph in enumerate(chunk.morphs[:-1]):  # 最後の形態素を含めない\n",
    "                if morph.pos1 == 'サ変接続' and chunk.morphs[i + 1].surface == 'を':\n",
    "                    sahen_wo = morph.surface + chunk.morphs[i + 1].surface\n",
    "                    break\n",
    "\n",
    "            if not sahen_wo:  # 該当しない場合は次の文節へ\n",
    "                continue\n",
    "\n",
    "            # 「サ変接続名詞+を」の係り先に動詞があるかを判定\n",
    "            verb = None\n",
    "            dst_chunk = sentence.chunks[chunk.dst] if chunk.dst != -1 else None\n",
    "            if dst_chunk:  # 係り先が存在する場合\n",
    "                for morph in dst_chunk.morphs:\n",
    "                    if morph.pos == '動詞':  # 動詞を探す\n",
    "                        verb = morph.base  # 動詞の基本形\n",
    "                        break\n",
    "\n",
    "            if not verb:  # 動詞がない場合は次の文節へ\n",
    "                continue\n",
    "\n",
    "            # 述語を生成\n",
    "            predicate = f\"{sahen_wo}{verb}\"\n",
    "\n",
    "            # 助詞と項の収集\n",
    "            cases_and_items = []\n",
    "            for src in dst_chunk.srcs:\n",
    "                if src == chunk.dst or src == sentence.chunks.index(chunk):  # 自身の文節は無視\n",
    "                    continue\n",
    "                src_chunk = sentence.chunks[src]\n",
    "                particles = [m.surface for m in src_chunk.morphs if m.pos == '助詞']\n",
    "                if particles:\n",
    "                    item = ''.join(m.surface for m in src_chunk.morphs)\n",
    "                    cases_and_items.append((particles[-1], item))\n",
    "\n",
    "            if cases_and_items:\n",
    "                # 助詞と項を辞書順にソート\n",
    "                cases_and_items.sort(key=lambda x: x[0])\n",
    "                cases = [ci[0] for ci in cases_and_items]\n",
    "                items = [ci[1] for ci in cases_and_items]\n",
    "\n",
    "                # 出力形式: 述語 + 助詞 + 項\n",
    "                line = f\"{predicate}\\t{' '.join(cases)}\\t{' '.join(items)}\"\n",
    "                print(line, file=f)\n",
    "                print(line)  # 確認用\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本大百科全書(ニッポニカ)』の -> 解説で -> 述べている\n",
      "解説で -> 述べている\n",
      "情報工学者通信工学者の -> 佐藤理史は -> 述べている\n",
      "佐藤理史は -> 述べている\n",
      "次のように -> 述べている\n"
     ]
    }
   ],
   "source": [
    "#48\n",
    "sentence=sentences[2]    \n",
    "for chunk in sentence.chunks:\n",
    "        # 文節に名詞を含む場合のみ処理\n",
    "        if any(morph.pos == \"名詞\" for morph in chunk.morphs):\n",
    "                path = []  # パスを格納するリスト\n",
    "                current_chunk = chunk\n",
    "\n",
    "                # 構文木の根に至るまでパスをたどる\n",
    "                while current_chunk:\n",
    "                    # 文節の表層形を取得し、記号を除外\n",
    "                    surface = ''.join(morph.surface for morph in current_chunk.morphs if morph.pos != '記号')\n",
    "\n",
    "                    # パスに追加\n",
    "                    path.append(surface)\n",
    "\n",
    "                    # 次の文節へ（係り先が -1 の場合は終了）\n",
    "                    if current_chunk.dst == -1:\n",
    "                        break\n",
    "                    current_chunk = sentence.chunks[current_chunk.dst]\n",
    "\n",
    "                # パスを \" -> \" で連結\n",
    "                path_str = \" -> \".join(path)\n",
    "\n",
    "                # 結果をコンソールに出力\n",
    "                print(path_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X大の ->  -> Yで\n",
      "X大の -> 解説で | Yの -> 佐藤理史は | 述べている\n",
      "X大の -> 解説で | Yは ->  | 述べている\n",
      "X大の -> 解説で | Yのに ->  | 述べている\n",
      "Xで ->  | Yの -> 佐藤理史は | 述べている\n",
      "Xで ->  | Yは ->  | 述べている\n",
      "Xで ->  | Yのに ->  | 述べている\n",
      "Xの ->  -> Yは\n",
      "Xの -> 佐藤理史は | Yのに ->  | 述べている\n",
      "Xは ->  | Yのに ->  | 述べている\n"
     ]
    }
   ],
   "source": [
    "#49\n",
    "from itertools import combinations\n",
    "import re\n",
    "\n",
    "sentence = sentences[2]\n",
    "nouns = []\n",
    "\n",
    "# 名詞を含む文節のインデックスを取得\n",
    "for i, chunk in enumerate(sentence.chunks):\n",
    "    if any(morph.pos == \"名詞\" for morph in chunk.morphs):  # 名詞を含む文節を抽出\n",
    "        nouns.append(i)\n",
    "\n",
    "# 名詞を含む文節ペアごとにパスを作成\n",
    "for i, j in combinations(nouns, 2):\n",
    "    path_I = []\n",
    "    path_J = []\n",
    "\n",
    "    # 文節iと文節jがどのように構文木上で繋がるかを確認\n",
    "    while i != j:\n",
    "        if i < j:  # 文節iの構文木経路上に文節jが存在する場合\n",
    "            path_I.append(i)\n",
    "            i = sentence.chunks[i].dst\n",
    "        else:  # 文節iの構文木経路上に文節jがない場合\n",
    "            path_J.append(j)\n",
    "            j = sentence.chunks[j].dst\n",
    "\n",
    "    # 文節iの構文木上に文節jが存在する場合\n",
    "    if len(path_J) == 0:\n",
    "        X = \"X\" + \"\".join([morph.surface for morph in sentence.chunks[path_I[0]].morphs if morph.pos != \"名詞\" and morph.pos != \"記号\"]) \n",
    "        Y = \"Y\" + \"\".join([morph.surface for morph in sentence.chunks[i].morphs if morph.pos != \"名詞\" and morph.pos != \"記号\"])\n",
    "        chunk_X = re.sub(\"X+\", \"X\", X)\n",
    "        chunk_Y = re.sub(\"Y+\", \"Y\", Y)\n",
    "        \n",
    "        # 文節iから文節jに至る経路を表示\n",
    "        path_ItoJ = [chunk_X] + [\"\".join(morph.surface for n in path_I[1:] for morph in sentence.chunks[n].morphs if morph.pos != \"記号\")] + [chunk_Y]\n",
    "        print(\" -> \".join(path_ItoJ))\n",
    "    \n",
    "    # 文節iの構文木上に文節jが存在しない場合\n",
    "    else:\n",
    "        X = \"X\" + \"\".join([morph.surface for morph in sentence.chunks[path_I[0]].morphs if morph.pos != \"名詞\" and morph.pos != \"記号\"]) \n",
    "        Y = \"Y\" + \"\".join([morph.surface for morph in sentence.chunks[path_J[0]].morphs if morph.pos != \"名詞\" and morph.pos != \"記号\"]) \n",
    "        chunk_X = re.sub(\"X+\", \"X\", X)\n",
    "        chunk_Y = re.sub(\"Y+\", \"Y\", Y)\n",
    "        chunk_k = \"\".join([morph.surface for morph in sentence.chunks[i].morphs if morph.pos != \"記号\"])\n",
    "\n",
    "        # 共通の文節で分けて表示\n",
    "        path_X = [chunk_X] + [\"\".join(morph.surface for n in path_I[1:] for morph in sentence.chunks[n].morphs if morph.pos != \"記号\")]\n",
    "        path_Y = [chunk_Y] + [\"\".join(morph.surface for n in path_J[1:] for morph in sentence.chunks[n].morphs if morph.pos != \"記号\")]\n",
    "        \n",
    "        # 結果を表示\n",
    "        print(\" | \".join([\" -> \".join(path_X), \" -> \".join(path_Y), chunk_k]))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
